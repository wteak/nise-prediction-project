{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94183065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --- [ 0. ê²½ë¡œ ë° ë³€ìˆ˜ ì„¤ì • ] ---\n",
    "print(\"--- [ 'One Universe' (Lag + sin/cos + íŠœë‹ V2) íŒŒì´í”„ë¼ì¸ ì‹œì‘ ] ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# --- ì›ë³¸ ë°ì´í„° ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ) ---\n",
    "CSV_TRAIN_PATH = 'train.csv'\n",
    "CSV_TEST_PATH = 'test.csv'\n",
    "SAMPLE_SUB_PATH = 'submission_sample.csv'\n",
    "\n",
    "# --- 'ìŠ¤ë§ˆíŠ¸' ìºì‹œ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ) ---\n",
    "FEATHER_TRAIN_PATH = 'train_original.feather'\n",
    "FEATHER_TEST_PATH = 'test_original.feather'\n",
    "\n",
    "# --- ìµœì¢… ì œì¶œ íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ ê²½ë¡œ) ---\n",
    "SUB_PATH = 'submission_Tuned_v2.csv' # ìƒˆ ì œì¶œ íŒŒì¼ ì´ë¦„\n",
    "\n",
    "# --- 'Y ë¶„ë¦¬' ëŒ€ìƒ ì»¬ëŸ¼ ---\n",
    "TARGET_COLS_TO_SEPARATE = ['nins', 'energy']\n",
    "\n",
    "# --- 'ë‹¨ì¼ ë³´ê°„' ëŒ€ìƒ ì»¬ëŸ¼ (91.6% NaN ì»¬ëŸ¼ë“¤) ---\n",
    "COLS_TO_INTERPOLATE = [\n",
    "    'appr_temp', 'ceiling', 'cloud_b', 'dew_point', 'precip_1h', \n",
    "    'pressure', 'real_feel_temp', 'real_feel_temp_shade', 'rel_hum', \n",
    "    'temp_b', 'uv_idx', 'vis', 'wind_chill_temp', 'wind_dir_b', \n",
    "    'wind_gust_spd', 'wind_spd_b', 'cloud_a', 'ground_press', \n",
    "    'humidity', 'rain', 'snow', 'temp_a', 'temp_max', 'temp_min', \n",
    "    'wind_dir_a', 'wind_spd_a'\n",
    "]\n",
    "\n",
    "# --- 'ë‹¨ì¼ êµ°ì§‘' ëŒ€ìƒ ì»¬ëŸ¼ ---\n",
    "CLUSTER_FEATURES = ['coord1', 'coord2']\n",
    "N_CLUSTERS = 20\n",
    "\n",
    "\n",
    "try:\n",
    "    # --- [ 1-3 ë‹¨ê³„: ìŠ¤ë§ˆíŠ¸ ë¡œë“œ, Y ë¶„ë¦¬, X ê²°í•© ] ---\n",
    "    print(\"\\n--- [ 1-3 ë‹¨ê³„: ìŠ¤ë§ˆíŠ¸ ë¡œë“œ, Y ë¶„ë¦¬, X ê²°í•© ] ---\")\n",
    "    if os.path.exists(FEATHER_TRAIN_PATH):\n",
    "        df_train = pd.read_feather(FEATHER_TRAIN_PATH)\n",
    "    else:\n",
    "        df_train = pd.read_csv(CSV_TRAIN_PATH, parse_dates=['time'])\n",
    "        df_train.to_feather(FEATHER_TRAIN_PATH)\n",
    "    if os.path.exists(FEATHER_TEST_PATH):\n",
    "        df_test = pd.read_feather(FEATHER_TEST_PATH)\n",
    "    else:\n",
    "        df_test = pd.read_csv(CSV_TEST_PATH, parse_dates=['time'])\n",
    "        df_test.to_feather(FEATHER_TEST_PATH)\n",
    "\n",
    "    print(\"--- [ 2. Y ë¶„ë¦¬ ] ---\")\n",
    "    y_train = df_train['nins'].copy()\n",
    "    cols_to_drop_train = [col for col in TARGET_COLS_TO_SEPARATE if col in df_train.columns]\n",
    "    df_train = df_train.drop(columns=cols_to_drop_train)\n",
    "    cols_to_drop_test = [col for col in TARGET_COLS_TO_SEPARATE if col in df_test.columns]\n",
    "    if cols_to_drop_test:\n",
    "         df_test = df_test.drop(columns=cols_to_drop_test)\n",
    "    print(f\"'y_train' ë°±ì—… ë° {TARGET_COLS_TO_SEPARATE} ì œê±° ì™„ë£Œ.\")\n",
    "\n",
    "    print(\"--- [ 3. X ê²°í•© (A/B ì„¸ê³„ í†µì¼) ] ---\")\n",
    "    df_full = pd.concat([df_train, df_test], ignore_index=True)\n",
    "    print(f\"'df_full' (One Universe) ìƒì„± ì™„ë£Œ. (ì´ {len(df_full)} í–‰)\")\n",
    "\n",
    "\n",
    "    # --- [ 4 ë‹¨ê³„: 'ë‹¨ì¼' ë³´ê°„ (ffill/bfill) ] ---\n",
    "    print(\"\\n--- [ 4 ë‹¨ê³„: 'ë‹¨ì¼' ë³´ê°„ (ffill/bfill) ] ---\")\n",
    "    cols_exist = [col for col in COLS_TO_INTERPOLATE if col in df_full.columns]\n",
    "    df_full[cols_exist] = df_full.groupby('pv_id')[cols_exist].ffill().bfill()\n",
    "    nan_check = df_full[cols_exist].isna().sum().sum()\n",
    "    print(f\"ë³´ê°„ ì™„ë£Œ. ë‚¨ì€ NaN ê°œìˆ˜: {nan_check}\")\n",
    "\n",
    "\n",
    "    # --- [ 5 ë‹¨ê³„: 'ë‹¨ì¼' êµ°ì§‘ ] ---\n",
    "    print(\"\\n--- [ 5 ë‹¨ê³„: 'ë‹¨ì¼' Scaler + KMeans êµ°ì§‘ ] ---\")\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(df_full[CLUSTER_FEATURES])\n",
    "    kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(scaled_features)\n",
    "    df_full['cluster_id'] = cluster_labels\n",
    "    print(\"'cluster_id' ìƒì„± ì™„ë£Œ.\")\n",
    "\n",
    "    \n",
    "    # --- [ 5.5 ë‹¨ê³„ (1ìˆœìœ„): Lag í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ] ---\n",
    "    print(\"\\n--- [ 5.5 ë‹¨ê³„ (1ìˆœìœ„): Lag í”¼ì²˜ ìƒì„± ] ---\")\n",
    "    lag_start_time = time.time()\n",
    "    for col in cols_exist: \n",
    "        df_full[f'{col}_lag30m'] = df_full.groupby('pv_id')[col].shift(6)\n",
    "        df_full[f'{col}_lag1h'] = df_full.groupby('pv_id')[col].shift(12)\n",
    "    print(\"Lag í”¼ì²˜ ìƒì„± ì™„ë£Œ. (52ê°œ ì‹ ê·œ ìƒì„±)\")\n",
    "    df_full = df_full.groupby('pv_id').bfill() \n",
    "    print(f\"Lag í”¼ì²˜ ë‹¨ê³„ ì™„ë£Œ. (ì†Œìš” ì‹œê°„: {time.time() - lag_start_time:.2f} ì´ˆ)\")\n",
    "\n",
    "    \n",
    "    # --- [ 5.6 ë‹¨ê³„ ('ì •ì œëœ' 2ìˆœìœ„): 'ì‹œê°„' í”¼ì²˜ ìƒì„± ] ---\n",
    "    print(\"\\n--- [ 5.6 ë‹¨ê³„ ('ì •ì œëœ' 2ìˆœìœ„): 'ì‹œê°„' í”¼ì²˜ ìƒì„± ] ---\")\n",
    "    df_full['hour'] = df_full['time'].dt.hour\n",
    "    df_full['day_of_year'] = df_full['time'].dt.dayofyear\n",
    "    df_full['month'] = df_full['time'].dt.month\n",
    "    df_full['weekday'] = df_full['time'].dt.weekday\n",
    "    df_full['hour_sin'] = np.sin(2 * np.pi * df_full['hour'] / 24.0)\n",
    "    df_full['hour_cos'] = np.cos(2 * np.pi * df_full['hour'] / 24.0)\n",
    "    print(\"'hour_sin', 'hour_cos' í”¼ì²˜ ìƒì„± ì™„ë£Œ.\")\n",
    "\n",
    "\n",
    "    # --- [ 6 ë‹¨ê³„: A(Train) / B(Test) ë¶„ë¦¬ ] ---\n",
    "    print(\"\\n--- [ 6 ë‹¨ê³„: A(Train) / B(Test) ë¶„ë¦¬ ] ---\")\n",
    "    X_train_processed = df_full[df_full['type'] == 'train'].copy()\n",
    "    X_test_processed = df_full[df_full['type'] == 'test'].copy()\n",
    "    print(f\"X_train: {X_train_processed.shape}, X_test: {X_test_processed.shape}\")\n",
    "\n",
    "    \n",
    "    # --- [ 7 ë‹¨ê³„: ìµœì¢… í”¼ì²˜ ì •ì˜ ] ---\n",
    "    print(\"\\n--- [ 7 ë‹¨ê³„: ìµœì¢… í”¼ì²˜ ë¦¬ìŠ¤íŠ¸ ì •ì˜ ] ---\")\n",
    "    FEATURES = list(X_train_processed.select_dtypes(include=np.number).columns)\n",
    "    COLS_TO_DROP_FINAL = ['nins', 'energy'] \n",
    "    FEATURES = [col for col in FEATURES if col not in COLS_TO_DROP_FINAL]\n",
    "    print(f\"ì´ {len(FEATURES)}ê°œì˜ í”¼ì²˜ë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "    # --- [ 8 ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ (LightGBM) - (íŠœë‹ V2) ] ---\n",
    "    print(\"\\n--- [ 8 ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ (LightGBM) - (íŠœë‹ V2) ] ---\")\n",
    "    \n",
    "    # [ íŠœë‹ V2: 127 + ê³¼ì í•© ë°©ì§€ 3ì¢… ]\n",
    "    lgb_params = {\n",
    "        'objective': 'mae',\n",
    "        'metric': 'mae',\n",
    "        'n_estimators': 10000,          # ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•´ ë„‰ë„‰í•˜ê²Œ\n",
    "        'learning_rate': 0.01,         # 53ì  ëª¨ë¸ê³¼ ë™ì¼ (ì •ë°€í•˜ê²Œ)\n",
    "        \n",
    "        # [ìˆ˜ì •ë¨] 1. ë” ë³µì¡í•œ ëª¨ë¸\n",
    "        'num_leaves': 127,             # 63 -> 127\n",
    "        \n",
    "        # [ì‹ ê·œ] 2. ê³¼ì í•© ë°©ì§€ (ì•”ê¸° ë°©ì§€)\n",
    "        'subsample': 0.8,              # 80%ì˜ í–‰ë§Œ ë¬´ì‘ìœ„ë¡œ ì‚¬ìš©\n",
    "        'colsample_bytree': 0.7,       # 70%ì˜ í”¼ì²˜ë§Œ ë¬´ì‘ìœ„ë¡œ ì‚¬ìš©\n",
    "        'min_data_in_leaf': 50,        # ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ë°ì´í„° ìˆ˜\n",
    "        \n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1,\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**lgb_params)\n",
    "    \n",
    "    print(f\"íŠœë‹ V2 ëª¨ë¸ (lr={lgb_params['learning_rate']}, leaves={lgb_params['num_leaves']}) í•™ìŠµ ì‹œì‘...\")\n",
    "    model.fit(X_train_processed[FEATURES], y_train,\n",
    "              eval_set=[(X_train_processed[FEATURES], y_train)], # ìì²´ ê²€ì¦\n",
    "              callbacks=[lgb.early_stopping(100, verbose=100)]) # ì¡°ê¸° ì¢…ë£Œ\n",
    "\n",
    "\n",
    "    # --- [ 9 ë‹¨ê³„: ì˜ˆì¸¡ (Prediction) - (ì •ì œë¨) ] ---\n",
    "    print(\"\\n--- [ 9 ë‹¨ê³„: ì˜ˆì¸¡ ] ---\")\n",
    "    \n",
    "    predictions = model.predict(X_test_processed[FEATURES])\n",
    "    \n",
    "    # í›„ì²˜ë¦¬ 1: ë°œì „ëŸ‰ì€ 0 ë¯¸ë§Œì´ ë  ìˆ˜ ì—†ìŒ (ìœ ì¼í•œ í›„ì²˜ë¦¬)\n",
    "    predictions[predictions < 0] = 0\n",
    "    print(\"ì˜ˆì¸¡ ì™„ë£Œ: (1) 0 ë¯¸ë§Œ ê°’ 0 ì²˜ë¦¬ ì™„ë£Œ.\")\n",
    "\n",
    "\n",
    "    # --- [ 10 ë‹¨ê³„: ì œì¶œ íŒŒì¼ ìƒì„± ] ---\n",
    "    print(\"\\n--- [ 10 ë‹¨ê³„: ì œì¶œ íŒŒì¼ ìƒì„± ] ---\")\n",
    "    \n",
    "    df_submission = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "    df_submission['nins'] = predictions\n",
    "    \n",
    "    df_submission.to_csv(SUB_PATH, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"\\n\\n--- [ 'One Universe' (íŠœë‹ V2) ì™„ë£Œ ] ---\")\n",
    "    print(f\"ìµœì¢… ì œì¶œ íŒŒì¼ì´ '{SUB_PATH}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"ì´ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f} ì´ˆ\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"[ì¹˜ëª…ì  ì˜¤ë¥˜] í•„ìˆ˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e.filename}\")\n",
    "except MemoryError:\n",
    "    print(\"[ì˜¤ë¥˜] ë©”ëª¨ë¦¬ ë¶€ì¡± (MemoryError).\")\n",
    "except Exception as e:\n",
    "    print(f\"[ì˜¤ë¥˜] íŒŒì´í”„ë¼ì¸ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd451f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Simple MAE Check] 1:1 í–‰ë‹¨ìœ„ ë¹„êµ ---\n",
      "\n",
      "========================================\n",
      " ğŸ“‰ ìµœì¢… MAE: 66.473928\n",
      "========================================\n",
      ">> [í™•ì¸] ì ìˆ˜(66.47392760948647)ê°€ ì˜ˆìƒë³´ë‹¤ ë†’ìŠµë‹ˆë‹¤. ë³´ê°„ë²• ì°¨ì´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# --- íŒŒì¼ ê²½ë¡œ ---\n",
    "TRUE_FILE = 'true.csv'                  # ì •ë‹µ íŒŒì¼\n",
    "SUB_FILE = 'submission_Final_User_Fact.csv' # ë°©ê¸ˆ ë§Œë“  ì œì¶œ íŒŒì¼\n",
    "\n",
    "print(\"--- [Simple MAE Check] 1:1 í–‰ë‹¨ìœ„ ë¹„êµ ---\")\n",
    "\n",
    "try:\n",
    "    # 1. íŒŒì¼ ë¡œë“œ (ì‹œê°„ íŒŒì‹± ì•ˆ í•¨ -> ì˜¤ì§ ê°’ë§Œ ë¹„êµ)\n",
    "    df_true = pd.read_csv(TRUE_FILE)\n",
    "    df_pred = pd.read_csv(SUB_FILE)\n",
    "\n",
    "    # 2. í–‰ ê°œìˆ˜ ê²€ì¦ (ì´ê²Œ ë‹¤ë¥´ë©´ ë¹„êµ ë¶ˆê°€)\n",
    "    if len(df_true) != len(df_pred):\n",
    "        print(f\"[ì˜¤ë¥˜] í–‰ ê°œìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤! True: {len(df_true)}, Pred: {len(df_pred)}\")\n",
    "        print(\"íŒŒì¼ì´ ì˜ëª» ìƒì„±ë˜ì—ˆê±°ë‚˜, true.csvê°€ ë‹¤ë¥¸ íŒŒì¼ì…ë‹ˆë‹¤.\")\n",
    "    \n",
    "    else:\n",
    "        # 3. MAE ê³„ì‚° (ìˆœì„œê°€ ê°™ë‹¤ê³  í™•ì‹ í•˜ë¯€ë¡œ ë°”ë¡œ ê³„ì‚°)\n",
    "        # nins ì»¬ëŸ¼ì˜ ê°’ë§Œ ì™ ë½‘ì•„ì„œ(values) ë¹„êµí•©ë‹ˆë‹¤. ì¸ë±ìŠ¤ë‚˜ ì‹œê°„ ì‹ ê²½ ì•ˆ ì”€.\n",
    "        y_true = df_true['nins'].values\n",
    "        y_pred = df_pred['nins'].values\n",
    "        \n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*40)\n",
    "        print(f\" ğŸ“‰ ìµœì¢… MAE: {mae:.6f}\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        # ê²°ê³¼ í•´ì„\n",
    "        if mae < 55:\n",
    "            print(\">> [ì„±ê³µ] ì •ìƒ ë²”ìœ„(53ì ëŒ€)ì…ë‹ˆë‹¤.\")\n",
    "        elif mae > 130:\n",
    "            print(\">> [ê²½ê³ ] ì ìˆ˜ê°€ ì—¬ì „íˆ 130ì ëŒ€ë¼ë©´, UTC/KST ì‹œì°¨ ë¬¸ì œì…ë‹ˆë‹¤.\")\n",
    "            print(\">> (0ì‹œ=ë‚®, 12ì‹œ=ë°¤ìœ¼ë¡œ ì˜ˆì¸¡ëœ ìƒí™©)\")\n",
    "        else:\n",
    "            print(f\">> [í™•ì¸] ì ìˆ˜({mae})ê°€ ì˜ˆìƒë³´ë‹¤ ë†’ìŠµë‹ˆë‹¤. ë³´ê°„ë²• ì°¨ì´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "except KeyError:\n",
    "    print(\"[ì˜¤ë¥˜] íŒŒì¼ì— 'nins' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"[ì˜¤ë¥˜] {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fb37ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Final Hybrid Pipeline] Data-Driven Context + Physics-Guided Logic ---\n",
      "\n",
      "[Step 1] ë°ì´í„° ë¡œë“œ ë° ë¬´ê²°ì„± í™•ë³´...\n",
      " -> í†µí•© ì™„ë£Œ. ì´ í–‰ ìˆ˜: 22075188\n",
      "\n",
      "[Step 2] í•˜ì´ë¸Œë¦¬ë“œ ë³´ê°„(Hybrid Interpolation) ìˆ˜í–‰...\n",
      "\n",
      "[Step 3] ì´ì¤‘ êµ°ì§‘í™” (Dual Clustering) ì „ëµ ì ìš©...\n",
      "\n",
      "[Step 4] í•˜ì´ë¸Œë¦¬ë“œ íŠ¹ì„± ê³µí•™ (Physics + Data)...\n",
      " -> ëª¨ë¸ ë¬¸ë§¥ ê°•í™”ë¥¼ ìœ„í•œ Lag ë³€ìˆ˜ ëŒ€ëŸ‰ ìƒì„± ì¤‘...\n",
      " -> íŠ¹ì„± í™•ì¥ ì™„ë£Œ. ì´ ë³€ìˆ˜ ê°œìˆ˜: 94\n",
      "\n",
      "[Step 5] í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 12.2 GiB for an array with shape (85, 19236948) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 170\u001b[39m\n\u001b[32m    167\u001b[39m mask_train = df_full[\u001b[33m'\u001b[39m\u001b[33mdataset_type\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    168\u001b[39m mask_test = df_full[\u001b[33m'\u001b[39m\u001b[33mdataset_type\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m X_train = \u001b[43mdf_full\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m X_test = df_full.loc[mask_test].copy()\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# í•™ìŠµ í”¼ì²˜ ì„ ì • (ìˆ˜ì¹˜í˜• + ë²”ì£¼í˜•)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:6830\u001b[39m, in \u001b[36mNDFrame.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m   6681\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   6682\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m, deep: bool_t | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> Self:\n\u001b[32m   6683\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   6684\u001b[39m \u001b[33;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[32m   6685\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   6828\u001b[39m \u001b[33;03m    dtype: int64\u001b[39;00m\n\u001b[32m   6829\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6830\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6831\u001b[39m     \u001b[38;5;28mself\u001b[39m._clear_item_cache()\n\u001b[32m   6832\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(data, axes=data.axes).__finalize__(\n\u001b[32m   6833\u001b[39m         \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mcopy\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   6834\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:604\u001b[39m, in \u001b[36mBaseBlockManager.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m    601\u001b[39m         res._blklocs = \u001b[38;5;28mself\u001b[39m._blklocs.copy()\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1788\u001b[39m, in \u001b[36mBlockManager._consolidate_inplace\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1783\u001b[39m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[32m   1784\u001b[39m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[32m   1785\u001b[39m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[32m   1786\u001b[39m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[32m   1787\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_consolidated():\n\u001b[32m-> \u001b[39m\u001b[32m1788\u001b[39m         \u001b[38;5;28mself\u001b[39m.blocks = \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m         \u001b[38;5;28mself\u001b[39m._is_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1790\u001b[39m         \u001b[38;5;28mself\u001b[39m._known_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2269\u001b[39m, in \u001b[36m_consolidate\u001b[39m\u001b[34m(blocks)\u001b[39m\n\u001b[32m   2267\u001b[39m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] = []\n\u001b[32m   2268\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[32m-> \u001b[39m\u001b[32m2269\u001b[39m     merged_blocks, _ = \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2270\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[32m   2271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2272\u001b[39m     new_blocks = extend_blocks(merged_blocks, new_blocks)\n\u001b[32m   2273\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2301\u001b[39m, in \u001b[36m_merge_blocks\u001b[39m\u001b[34m(blocks, dtype, can_consolidate)\u001b[39m\n\u001b[32m   2298\u001b[39m     new_values = bvals2[\u001b[32m0\u001b[39m]._concat_same_type(bvals2, axis=\u001b[32m0\u001b[39m)\n\u001b[32m   2300\u001b[39m argsort = np.argsort(new_mgr_locs)\n\u001b[32m-> \u001b[39m\u001b[32m2301\u001b[39m new_values = \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   2302\u001b[39m new_mgr_locs = new_mgr_locs[argsort]\n\u001b[32m   2304\u001b[39m bp = BlockPlacement(new_mgr_locs)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 12.2 GiB for an array with shape (85, 19236948) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import joblib\n",
    "\n",
    "# [Configuration] ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥ ë°©ì§€ (Clean Output)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"--- [Final Hybrid Pipeline] Data-Driven Context + Physics-Guided Logic ---\")\n",
    "start_total = time.time()\n",
    "\n",
    "# ==============================================================================\n",
    "# [Step 0] í™˜ê²½ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "# ==============================================================================\n",
    "CSV_TRAIN = 'train.csv'\n",
    "CSV_TEST = 'test.csv'\n",
    "SAMPLE_SUB = 'submission_sample.csv'\n",
    "OUTPUT_FILE = 'submission_Final_Hybrid.csv'\n",
    "\n",
    "# [Hyperparameters] ì‹¤í—˜ ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹± íŠœë‹ (Heuristic Tuning)\n",
    "# í’ë¶€í•œ ì‹œì°¨(Lag) ë³€ìˆ˜ í•™ìŠµì„ ìœ„í•´ ëª¨ë¸ ë³µì¡ë„(num_leaves)ë¥¼ ìœ ì§€í•˜ë˜,\n",
    "# ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ subsample(í–‰ ìƒ˜í”Œë§)ê³¼ colsample(ì—´ ìƒ˜í”Œë§) ì ìš©\n",
    "LGB_PARAMS = {\n",
    "    'objective': 'mae',\n",
    "    'metric': 'mae',\n",
    "    'n_estimators': 3000,\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves': 127,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# [Step 1] ë°ì´í„° ë¡œë“œ ë° í†µí•© (One-Universe)\n",
    "# ==============================================================================\n",
    "print(\"\\n[Step 1] ë°ì´í„° ë¡œë“œ ë° ë¬´ê²°ì„± í™•ë³´...\")\n",
    "\n",
    "# ë‚ ì§œ íŒŒì‹±ì„ í¬í•¨í•œ ê³ ì† ë¡œë“œ\n",
    "df_train = pd.read_csv(CSV_TRAIN, parse_dates=['time'])\n",
    "df_test = pd.read_csv(CSV_TEST, parse_dates=['time'])\n",
    "\n",
    "# [Safety] ì œì¶œ ì‹œ ìˆœì„œ ì„ì„ ë°©ì§€ë¥¼ ìœ„í•œ ì›ë³¸ ì¸ë±ìŠ¤ ë°±ì—…\n",
    "df_test['original_index'] = df_test.index\n",
    "\n",
    "# íƒ€ê²Ÿ(Target) ë¶„ë¦¬\n",
    "y_train = df_train['nins'].copy()\n",
    "\n",
    "# Data Leakage ë°©ì§€ë¥¼ ìœ„í•´ íƒ€ê²Ÿ ë° íŒŒìƒ ì»¬ëŸ¼ ì œê±°\n",
    "drop_cols = ['nins', 'energy']\n",
    "df_train.drop(columns=[c for c in drop_cols if c in df_train.columns], inplace=True)\n",
    "df_test.drop(columns=[c for c in drop_cols if c in df_test.columns], inplace=True)\n",
    "\n",
    "# ë°ì´í„°ì…‹ êµ¬ë¶„ íƒœê·¸\n",
    "df_train['dataset_type'] = 'train'\n",
    "df_test['dataset_type'] = 'test'\n",
    "\n",
    "# [Strategy] One-Universe í†µí•© íŒŒì´í”„ë¼ì¸\n",
    "# Train/Testì˜ ìŠ¤ì¼€ì¼ë§ ë° êµ°ì§‘í™” ê¸°ì¤€ í†µì¼ì„ ìœ„í•´ ë³‘í•© ìˆ˜í–‰\n",
    "df_full = pd.concat([df_train, df_test], ignore_index=True)\n",
    "print(f\" -> í†µí•© ì™„ë£Œ. ì´ í–‰ ìˆ˜: {len(df_full)}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# [Step 2] ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (í•˜ì´ë¸Œë¦¬ë“œ ë³´ê°„)\n",
    "# ==============================================================================\n",
    "print(\"\\n[Step 2] í•˜ì´ë¸Œë¦¬ë“œ ë³´ê°„(Hybrid Interpolation) ìˆ˜í–‰...\")\n",
    "\n",
    "cols_weather = [\n",
    "    'appr_temp', 'ceiling', 'cloud_b', 'dew_point', 'precip_1h', 'pressure', \n",
    "    'real_feel_temp', 'real_feel_temp_shade', 'rel_hum', 'temp_b', 'uv_idx', 'vis', \n",
    "    'wind_chill_temp', 'wind_dir_b', 'wind_gust_spd', 'wind_spd_b', 'cloud_a', \n",
    "    'ground_press', 'humidity', 'rain', 'snow', 'temp_a', 'temp_max', 'temp_min', \n",
    "    'wind_dir_a', 'wind_spd_a'\n",
    "]\n",
    "# ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§\n",
    "cols_exist = [c for c in cols_weather if c in df_full.columns]\n",
    "\n",
    "# [Logic] ë¬¼ë¦¬ì  ì—°ì†ì„±(Continuity) ë³´ì¡´ì„ ìœ„í•´ ë°œì „ì†Œë³„ ì‹œê°„ìˆœ ì •ë ¬\n",
    "df_full.sort_values(by=['pv_id', 'time'], inplace=True)\n",
    "\n",
    "# [Imputation] ì‹œê³„ì—´ ì¶”ì„¸ë¥¼ ëŠì§€ ì•Šê¸° ìœ„í•´ Forward/Backward Fill ì ìš©\n",
    "df_full[cols_exist] = df_full.groupby('pv_id')[cols_exist].ffill().bfill()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# [Step 3] ì´ì¤‘ êµ°ì§‘í™” (Dual Clustering)\n",
    "# ==============================================================================\n",
    "print(\"\\n[Step 3] ì´ì¤‘ êµ°ì§‘í™” (Dual Clustering) ì „ëµ ì ìš©...\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 3-1. ìœ„ì¹˜ êµ°ì§‘ (Location Cluster) - Micro Segmentation\n",
    "# ì§€í˜•ì  íŠ¹ì„±(ìŒì˜, ê³ ë„ ë“±)ì„ ë°˜ì˜í•˜ê¸° ìœ„í•´ K=20ìœ¼ë¡œ ì„¸ë¶„í™”\n",
    "loc_features = ['coord1', 'coord2']\n",
    "scaled_loc = scaler.fit_transform(df_full[loc_features])\n",
    "kmeans_loc = KMeans(n_clusters=20, random_state=42, n_init=10)\n",
    "df_full['cluster_loc'] = kmeans_loc.fit_predict(scaled_loc)\n",
    "df_full['cluster_loc'] = df_full['cluster_loc'].astype('category')\n",
    "\n",
    "# 3-2. ê¸°í›„ êµ°ì§‘ (Climate Cluster) - Macro Segmentation\n",
    "# ì§€ì—­ ì „ë°˜ì˜ ê±°ì‹œì  ê¸°ìƒ íŒ¨í„´ì„ ë°˜ì˜í•˜ê¸° ìœ„í•´ K=10 ì„¤ì •\n",
    "clim_features = ['temp_a', 'humidity', 'wind_spd_a', 'pressure', 'vis']\n",
    "target_climate = [c for c in clim_features if c in df_full.columns]\n",
    "pv_stats = df_full.groupby('pv_id')[target_climate].mean() # ë°œì „ì†Œë³„ ê¸°í›„ í”„ë¡œí•„\n",
    "\n",
    "scaled_cli = scaler.fit_transform(pv_stats)\n",
    "kmeans_cli = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "pv_labels = kmeans_cli.fit_predict(scaled_cli)\n",
    "\n",
    "# ë§¤í•‘ (Mapping)\n",
    "climate_map = dict(zip(pv_stats.index, pv_labels))\n",
    "df_full['cluster_climate'] = df_full['pv_id'].map(climate_map).astype('category')\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# [Step 4] íŠ¹ì„± ê³µí•™ (ë¬¼ë¦¬ ê³µì‹ + Rich Context)\n",
    "# ==============================================================================\n",
    "print(\"\\n[Step 4] í•˜ì´ë¸Œë¦¬ë“œ íŠ¹ì„± ê³µí•™ (Physics + Data)...\")\n",
    "\n",
    "# 4-1. ì‹œê°„ ë³€ìˆ˜ (Cyclical Encoding)\n",
    "df_full['hour'] = df_full['time'].dt.hour\n",
    "df_full['minute'] = df_full['time'].dt.minute\n",
    "df_full['day_of_year'] = df_full['time'].dt.dayofyear\n",
    "\n",
    "# ì‹œê°„ì˜ ì—°ì†ì„±ì„ ìœ„í•´ Sin/Cos ë³€í™˜\n",
    "df_full['hour_sin'] = np.sin(2 * np.pi * df_full['hour'] / 24.0)\n",
    "df_full['hour_cos'] = np.cos(2 * np.pi * df_full['hour'] / 24.0)\n",
    "\n",
    "# 4-2. [Physics] íƒœì–‘ ê¸°í•˜í•™(Solar Geometry) ì‹œë®¬ë ˆì´ì…˜\n",
    "# ë‚ ì§œì™€ ì‹œê°„ì— ë”°ë¥¸ ì´ë¡ ì  íƒœì–‘ ì—ë„ˆì§€ ê°€ìš©ëŸ‰(Daylight Cosine) ê³„ì‚°\n",
    "# ê°€ì •: í•œêµ­ ì‹œê°„(KST) ê¸°ì¤€ ë‚¨ì¤‘ ì‹œê°(Solar Noon) ì•½ 12:00\n",
    "solar_noon = 12.0 \n",
    "day_duration = 12.0 + 2.5 * np.sin(2 * np.pi * (df_full['day_of_year'] - 80) / 365.25)\n",
    "\n",
    "df_full['decimal_hour'] = df_full['hour'] + df_full['minute'] / 60.0\n",
    "scaled_time = (df_full['decimal_hour'] - solar_noon) * np.pi / (day_duration / 2)\n",
    "\n",
    "# 'daylight_cosine'ì€ Step 7ì˜ ì•¼ê°„ ë§ˆìŠ¤í‚¹(Night Masking)ì— í™œìš©ë¨\n",
    "df_full['daylight_cosine'] = np.cos(scaled_time) \n",
    "\n",
    "# 4-3. [Data-Driven] ëŒ€ê·œëª¨ ì‹œì°¨(Lag) ë³€ìˆ˜ ìƒì„±\n",
    "# ëª¨ë¸ì—ê²Œ í’ë¶€í•œ ë¬¸ë§¥(Context)ì„ ì œê³µí•˜ê¸° ìœ„í•´ 26ê°œ ëª¨ë“  ì„¼ì„œ ë³€ìˆ˜ì˜ ê³¼ê±° ë°ì´í„° ìƒì„±\n",
    "print(\" -> ëª¨ë¸ ë¬¸ë§¥ ê°•í™”ë¥¼ ìœ„í•œ Lag ë³€ìˆ˜ ëŒ€ëŸ‰ ìƒì„± ì¤‘...\")\n",
    "\n",
    "for col in cols_exist:\n",
    "    # Lag 6 (30ë¶„ ì „), Lag 12 (1ì‹œê°„ ì „)\n",
    "    df_full[f'{col}_lag30m'] = df_full.groupby('pv_id')[col].shift(6)\n",
    "    df_full[f'{col}_lag1h'] = df_full.groupby('pv_id')[col].shift(12)\n",
    "\n",
    "print(f\" -> íŠ¹ì„± í™•ì¥ ì™„ë£Œ. ì´ ë³€ìˆ˜ ê°œìˆ˜: {df_full.shape[1]}\")\n",
    "\n",
    "# Lag ìƒì„±ìœ¼ë¡œ ì¸í•œ ì´ˆê¸° ê²°ì¸¡ì¹˜ëŠ” Backfillë¡œ ì²˜ë¦¬\n",
    "df_full = df_full.groupby('pv_id').bfill()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# [Step 5] ë°ì´í„° ë¶„ë¦¬ (Split)\n",
    "# ==============================================================================\n",
    "print(\"\\n[Step 5] í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬...\")\n",
    "mask_train = df_full['dataset_type'] == 'train'\n",
    "mask_test = df_full['dataset_type'] == 'test'\n",
    "\n",
    "X_train = df_full.loc[mask_train].copy()\n",
    "X_test = df_full.loc[mask_test].copy()\n",
    "\n",
    "# í•™ìŠµ í”¼ì²˜ ì„ ì • (ìˆ˜ì¹˜í˜• + ë²”ì£¼í˜•)\n",
    "feats_num = list(X_train.select_dtypes(include=['number']).columns)\n",
    "feats_cat = list(X_train.select_dtypes(include=['category']).columns)\n",
    "features = feats_num + feats_cat\n",
    "\n",
    "# í•™ìŠµ ë°©í•´ ì»¬ëŸ¼(ë©”íƒ€ ë°ì´í„°) ì œì™¸\n",
    "exclude_cols = {'nins', 'energy', 'pv_id', 'dataset_type', 'time', 'original_index'}\n",
    "features = [f for f in features if f not in exclude_cols]\n",
    "cat_feats = [f for f in feats_cat if f in features]\n",
    "\n",
    "print(f\" -> ìµœì¢… í•™ìŠµ ë³€ìˆ˜ ê°œìˆ˜: {len(features)}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# [Step 6] ëª¨ë¸ í•™ìŠµ (LightGBM)\n",
    "# ==============================================================================\n",
    "print(\"\\n[Step 6] LightGBM ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "model = lgb.LGBMRegressor(**LGB_PARAMS)\n",
    "\n",
    "model.fit(\n",
    "    X_train[features], y_train,\n",
    "    eval_set=[(X_train[features], y_train)],\n",
    "    categorical_feature=cat_feats,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "        lgb.log_evaluation(period=1000)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# [Step 7] ì˜ˆì¸¡ ë° ë¬¼ë¦¬ì  í›„ì²˜ë¦¬ (Safety Mechanisms)\n",
    "# ==============================================================================\n",
    "print(\"\\n[Step 7] ì¶”ë¡  ë° ì•ˆì „ì¥ì¹˜ ì ìš©...\")\n",
    "preds = model.predict(X_test[features])\n",
    "\n",
    "# 7-1. ê¸°ë³¸ ë³´ì • (ìŒìˆ˜ ë°œì „ëŸ‰ ì œê±°)\n",
    "preds = np.maximum(preds, 0)\n",
    "\n",
    "# 7-2. [Physics] ì•¼ê°„ ë§ˆìŠ¤í‚¹ (Daylight Masking)\n",
    "# 'daylight_cosine' ê°’ì´ 0 ì´í•˜(í•´ì§)ì¸ ê²½ìš° ê°•ì œë¡œ 0 ì²˜ë¦¬í•˜ì—¬ ë¬¼ë¦¬ì  ì •í•©ì„± í™•ë³´\n",
    "if 'daylight_cosine' in X_test.columns:\n",
    "    # ë¶€ë™ì†Œìˆ˜ì  ì˜¤ì°¨ ê³ ë ¤í•œ ì„ê³„ê°’ ì„¤ì •\n",
    "    mask_night = X_test['daylight_cosine'] <= 0.0001\n",
    "    preds[mask_night] = 0\n",
    "    print(f\" -> [í›„ì²˜ë¦¬] ë¬¼ë¦¬ì  ë°¤ ì‹œê°„ëŒ€ {sum(mask_night)}ê±´ì„ 0ìœ¼ë¡œ ë³´ì •í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 7-3. [Safety] ì¸ë±ìŠ¤ ë³µì› (Index Restoration)\n",
    "# ì œì¶œ íŒŒì¼ì˜ ìˆœì„œê°€ ì›ë³¸ test.csvì™€ ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•˜ë„ë¡ ì¬ì •ë ¬\n",
    "results = pd.DataFrame({\n",
    "    'nins': preds,\n",
    "    'original_index': X_test['original_index'].values\n",
    "})\n",
    "\n",
    "results.sort_values(by='original_index', inplace=True)\n",
    "\n",
    "# ==============================================================================\n",
    "# [Step 8] ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "# ==============================================================================\n",
    "sub = pd.read_csv(SAMPLE_SUB)\n",
    "sub['nins'] = results['nins'].values\n",
    "sub.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"\\n[ì„±ê³µ] ìµœì¢… ì œì¶œ íŒŒì¼ì´ '{OUTPUT_FILE}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "print(f\"ì´ ì†Œìš” ì‹œê°„: {time.time() - start_total:.2f} ì´ˆ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
