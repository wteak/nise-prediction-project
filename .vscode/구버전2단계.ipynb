{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e3274",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     46\u001b[39m processed_chunks = []\u001b[38;5;66;03m#빈 리스트 생성\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# chunksize 옵션을 사용하여 파일을 청크 단위로 순회\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRAW_FILE_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHUNK_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpython\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;66;43;03m#오류줄이기위해 엔진과 구분자 명시적 지정\u001b[39;49;00m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessed_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_data_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;43;03m#아까정의한 함수 이용 가공한 값들 리스트에 저장\u001b[39;49;00m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessed_chunks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_chunk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#청크 계속 누적시켜서 합치기위한 사전 만남의 광장\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1843\u001b[39m, in \u001b[36mTextFileReader.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1841\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> DataFrame:\n\u001b[32m   1842\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1843\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1844\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   1845\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1985\u001b[39m, in \u001b[36mTextFileReader.get_chunk\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m   1983\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m   1984\u001b[39m     size = \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m.nrows - \u001b[38;5;28mself\u001b[39m._currow)\n\u001b[32m-> \u001b[39m\u001b[32m1985\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:252\u001b[39m, in \u001b[36mPythonParser.read\u001b[39m\u001b[34m(self, rows)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(\n\u001b[32m    247\u001b[39m     \u001b[38;5;28mself\u001b[39m, rows: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    248\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\n\u001b[32m    249\u001b[39m     Index | \u001b[38;5;28;01mNone\u001b[39;00m, Sequence[Hashable] | MultiIndex, Mapping[Hashable, ArrayLike]\n\u001b[32m    250\u001b[39m ]:\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m         content = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    254\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._first_chunk:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:1127\u001b[39m, in \u001b[36mPythonParser._get_lines\u001b[39m\u001b[34m(self, rows)\u001b[39m\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m row_ct < rows:\n\u001b[32m   1124\u001b[39m     \u001b[38;5;66;03m# assert for mypy, data is Iterator[str] or None, would\u001b[39;00m\n\u001b[32m   1125\u001b[39m     \u001b[38;5;66;03m# error in next\u001b[39;00m\n\u001b[32m   1126\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1127\u001b[39m     new_row = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skipfunc(offset + row_index):\n\u001b[32m   1129\u001b[39m         row_ct += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:322\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 1. 라이브러리 임포트\n",
    "# ==============================================================================\n",
    "import pvlib#태양광 발전 전문 태양 위치 등 근데 위동경도가 아니라 과연 하는게 더 나을지 의문\n",
    "import pandas as pd\n",
    "import numpy as np#원활한 계산을 위해 전체적으로 적용됨 pands lgb둘다\n",
    "import gc#쓸데없는 메모리 회수처리\n",
    "import os#윈도우와 상호작용(중복된거있으면 건너뛸수있게)\n",
    "\n",
    "# 2. 설정 변수 정의\n",
    "# ==============================================================================\n",
    "# 파일 경로, 청크 크기 등 주요 설정을 상단에 정의하여 관리 용이성을 높입니다.\n",
    "RAW_FILE_PATH = 'train.csv'#원본 데이터 파일 경로(변수통해서 지정)\n",
    "CHUNK_SIZE = 5000000  # 메모리 상황에 맞춰 조절 (예: 50만 행)일단 만으로\n",
    "TARGET = 'nins'\n",
    "# 모델 학습에 사용할 최종 특성 리스트\n",
    "FEATURES = [#'ground_press', 'wind_gust_spd', 'wind_chill_temp', 'rel_hum', #'type', 'nins', #energe 까지 7개 제외    26개\n",
    "    'appr_temp', 'ceiling', 'cloud_b', 'dew_point', 'precip_1h', 'pressure',\n",
    "     'temp_b', 'uv_idx', 'vis', 'wind_dir_b', 'wind_spd_b',\n",
    "    'cloud_a', 'humidity', 'rain', 'snow', 'temp_a', 'wind_dir_a', 'wind_spd_a',\n",
    "    'coord1', 'coord2', 'time',\n",
    "    'pv_id', # 발전소 ID를 특성으로 사용할 예정\n",
    "    'real_feel_temp_shade', 'temp_max', 'temp_min', 'real_feel_temp'#밑거름이 될 칼럼들\n",
    "]\n",
    "# 3. 데이터 처리 및 피처 엔지니어링 함수\n",
    "\n",
    "def process_data_chunk(chunk_df):\n",
    "    # 'pv_id'를 메모리 효율적인 category 타입으로 변환\n",
    "    chunk_df['pv_id'] = chunk_df['pv_id'].astype('category')#반복되는 발전소번호등 간단히 저장(ex:pv_id1=1)\n",
    "    \n",
    "    # 함수 내부에 추가 #메모리최적화\n",
    "    for col in chunk_df.select_dtypes(include=['float64']).columns:#float64타입선택(colums는 선택된 열들 col로봔한)\n",
    "        chunk_df[col] = chunk_df[col].astype('float32')#32로 변환해서 col(정밀도 소폭감소 차이별로)\n",
    "    for col in chunk_df.select_dtypes(include=['int64']).columns:\n",
    "        chunk_df[col] = chunk_df[col].astype('int32')#정수형도 마찬가지\n",
    "#들어온 청크 싹 변환하고 밑에서 그변환값 이용해서 새로운 변수 생성\n",
    "\n",
    "    # 날짜/시간으로 변환 (erros는 혹시모를 오류방지(아예비어있거나그런)->자료에 그런거 없는거 같긴한데 혹시모르니)\n",
    "    chunk_df['time'] = pd.to_datetime(chunk_df['time'], errors='coerce')\n",
    "    \n",
    "    # 최종적으로 사용할 FEATURES 리스트에 있는 열들만 선택하여 반환\n",
    "    final_features = [f for f in FEATURES if f in chunk_df.columns]\n",
    "    return chunk_df[final_features + [TARGET]]#추가로 얘는 필요+1\n",
    "\n",
    "# --=함수정의 완료\n",
    "# 4. 대용량 데이터 로딩 및 전처리 실행\n",
    "processed_chunks = []#빈 리스트 생성\n",
    "# chunksize 옵션을 사용하여 파일을 청크 단위로 순회\n",
    "for chunk in pd.read_csv(RAW_FILE_PATH, chunksize=CHUNK_SIZE, engine='python', sep=','):#오류줄이기위해 엔진과 구분자 명시적 지정\n",
    "    processed_chunk = process_data_chunk(chunk)#아까정의한 함수 이용 가공한 값들 리스트에 저장\n",
    "    processed_chunks.append(processed_chunk)#청크 계속 누적시켜서 합치기위한 사전 만남의 광장\n",
    "\n",
    "df = pd.concat(processed_chunks, ignore_index=True)# 처리된 모든 청크를 하나의 데이터프레임으로 결합\n",
    "del processed_chunks#필요없어진 리스트 삭제\n",
    "gc.collect()#메모리 회수\n",
    "print(\"기본 처리 및 통합 완료.\")\n",
    "#--------------------------------------------------------------------------------------------------------- 최적화 파일불러오기\n",
    "\n",
    "# 5. 추가 피처 엔지니어링 \n",
    "print(\"\\n고급 피처 엔지니어링을 시작합니다...\")\n",
    "df.sort_values(by=['pv_id', 'time'], inplace=True)#혹시 모를 오류로 인해 동일한 열(Column)이 두 번 생성되는 것을 방지하는 **'방어용 코드\n",
    "# 기존의 중복되거나 순서가 섞인 인덱스가 있다면 버리고, 0부터 시작하는 새로운 인덱스를 부여(마찬가지 방어용)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df = df.loc[:, ~df.columns.duplicated(keep='first')]#중복된 칼럼 제거(앞에꺼 살리고 뒤에꺼 버림)\n",
    "\n",
    "#--------1.위치기반--------------------------------------------------------------------------------------------------------\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "print(\"위치 좌표(coord)를 기반으로 발전소 군집(Cluster) 특성을 생성합니다...\")\n",
    "#발전소별 평균 일사량 추정 1년치 전체\n",
    "pv_id_stats = df.groupby('pv_id')[['nins']].mean().reset_index() #1. 발전소와 일사량\n",
    "pv_id_stats.rename(columns={'nins': 'avg_nins'}, inplace=True)\n",
    "unique_locations = df[['pv_id', 'coord1', 'coord2']].drop_duplicates(subset=['pv_id'])# 2 발전소와 위치만 존재\n",
    "\n",
    "# 3. ★(수정)★ 고유 위치에 '평균 nins' 정보 병합\n",
    "unique_locations = pd.merge(unique_locations, pv_id_stats, on='pv_id', how='left') #1,2를 발전소 기준으로 합치는 코드\n",
    "\n",
    "# 4. ★(수정)★ KMeans에 사용할 특징 리스트 정의 (위치 + 지형 결과)\n",
    "cluster_features = ['coord1', 'coord2', 'avg_nins']\n",
    "# (결측치가 있으면 0으로 채움)\n",
    "unique_locations[cluster_features] = unique_locations[cluster_features].fillna(0)#일몰된거겠지\n",
    "\n",
    "# 5. ★(신규)★ StandardScaler로 특징들의 스케일 맞추기\n",
    "# (coord와 avg_nins의 단위(스케일)가 다르므로, 공평한 비교를 위해 필수)\n",
    "print(\"StandardScaler로 coord와 avg_nins의 단위를 통일합니다...\")\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(unique_locations[cluster_features])\n",
    "\n",
    "# 6. KMeans 모델로 19개의 '스마트 군집' 생성 (★10개 -> 19개★)\n",
    "print(f\"KMeans(n_clusters=19)로 {len(unique_locations)}개의 발전소를 3차원 공간에서 그룹화합니다...\")\n",
    "kmeans = KMeans(n_clusters=19, random_state=42, n_init=10)\n",
    "unique_locations['location_cluster'] = kmeans.fit_predict(scaled_features)\n",
    "    \n",
    "\n",
    "print(\"메모리 효율적인 .map()을 위해 'cluster_map'을 생성합니다...\")\n",
    "# (예: {100: 0, 101: 3, 102: 0, ...})\n",
    "cluster_map = unique_locations.set_index('pv_id')['location_cluster']\n",
    "\n",
    "# 4. ★(수정)★ .merge() 대신 .map()을 사용하여 열을 추가 (메모리 사용량 훨씬 적음)\n",
    "print(\".map()을 사용하여 'location_cluster' 열을 추가합니다...\")\n",
    "df['location_cluster'] = df['pv_id'].map(cluster_map)\n",
    "df['location_cluster'] = df['location_cluster'].astype('category')\n",
    "print(\"위치 군집 매핑 완료.\")\n",
    "\n",
    "\n",
    "#시간관련을 먼저하는게  클러스터별 칼럼 설정? 을 위해\n",
    "df['minute'] = df['time'].dt.minute\n",
    "df['hour'] = df['time'].dt.hour\n",
    "df['decimal_hour'] = df['hour'] + (df['minute'] / 60.0)\n",
    "df['day_of_year'] = df['time'].dt.dayofyear\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['decimal_hour'] / 24.0)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['decimal_hour'] / 24.0)\n",
    "df['day_of_year_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365.25)\n",
    "df['day_of_year_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365.25)\n",
    "df['seasonal_hour'] = df['day_of_year_sin'] * df['hour_sin']\n",
    "\n",
    "#--------3. ★★★ '맵' 파일 2종 생성 및 'Anomaly' 특징 적용 ★★★ -----------------------------------\n",
    "\n",
    "# ★★★ 3A. '일출/일몰 맵' 생성 (nins > 0 사용) ★★★\n",
    "print(\"클러스터별 '개인화 일출/일몰 맵'을 생성합니다...\")\n",
    "daytime_df = df[df['nins'] > 0].copy()\n",
    "cluster_times = daytime_df.groupby('location_cluster')['decimal_hour'].agg(['min', 'max']).reset_index()\n",
    "cluster_times.rename(columns={'min': 'sunrise_decimal', 'max': 'sunset_decimal'}, inplace=True)\n",
    "cluster_times['sunrise_decimal'] += (5/60) # 5분 마진\n",
    "cluster_times['sunset_decimal'] -= (5/60)  # 5분 마진\n",
    "cluster_times.to_csv('cluster_sun_times_map.csv', index=False)\n",
    "print(\"일출/일몰 맵 저장 완료.\")\n",
    "# (생성된 맵 특징을 train 데이터에도 병합 - 학습에 사용)\n",
    "df = pd.merge(df, cluster_times, on='location_cluster', how='left')\n",
    "\n",
    "# ★★★ 3B. '개인화 기준선(Anomaly) 맵' 생성 ★★★\n",
    "print(\"GroupBy 개인화 기준선 맵 생성 중...\")\n",
    "base_df = df.groupby(['location_cluster', 'hour'])[['cloud_a', 'uv_idx', 'temp_a','humidity']].mean().reset_index()\n",
    "base_df.rename(columns={\n",
    "    'cloud_a': 'cluster_hour_cloud_mean',\n",
    "    'uv_idx': 'cluster_hour_uv_mean',\n",
    "    'temp_a': 'cluster_hour_temp_mean',\n",
    "    'humidity': 'cluster_hour_humidity_mean'\n",
    "}, inplace=True)\n",
    "base_df.to_csv('cluster_hour_means_map.csv', index=False)\n",
    "print(\"개인화 기준선 맵 저장 완료.\")\n",
    "\n",
    "# ★★★ 3C. Anomaly 특징을 train 데이터에 적용 ★★★\n",
    "print(\"'평소 대비 이상 기후' (Anomaly) 특징 생성 중...\")\n",
    "df = pd.merge(df, base_df, on=['location_cluster', 'hour'], how='left')\n",
    "df['cloud_anomaly'] = df['cloud_a'] - df['cluster_hour_cloud_mean']\n",
    "df['uv_anomaly'] = df['uv_idx'] - df['cluster_hour_uv_mean']\n",
    "df['temp_anomaly'] = df['temp_a'] - df['cluster_hour_temp_mean']\n",
    "df['humidity_anomaly'] = df['humidity'] - df['cluster_hour_humidity_mean'] \n",
    "\n",
    "\n",
    "\n",
    "# --- 2.weather 날씨 '추세' 특성 생성-------------------------------------------------------------------------------------- \n",
    "print(\"날씨 추세(Rolling) 특성 생성 중...\")\n",
    "g = df.groupby('pv_id')\n",
    "#추세\n",
    "df['uv_idx_rolling_12'] = g['uv_idx'].shift(1).rolling(12, min_periods=1).mean()\n",
    "df['uv_idx_lag_12'] = g['uv_idx'].shift(12)\n",
    "df['humidity_rolling_12'] = g['humidity'].shift(1).rolling(12, min_periods=1).mean()\n",
    "df['humidity_lag_12'] = g['humidity'].shift(12)\n",
    "df['pressure_rolling_12'] = g['pressure'].shift(1).rolling(12, min_periods=1).mean()\n",
    "df['pressure_lag_12'] = g['pressure'].shift(12)#UV,습도기압추세추가 기존것과 다르게 과거값이라 상관없음\n",
    "df['cloud_a_rolling_12'] = g['cloud_a'].shift(1).rolling(12, min_periods=1).mean()#1시간 전 구름량의 1시간 이동평균\n",
    "df['temp_a_rolling_12'] = g['temp_a'].shift(1).rolling(12, min_periods=1).mean()#1시간 전 기온의 1시간 이동평균\n",
    "df['cloud_a_lag_12'] = g['cloud_a'].shift(12) #1시간 전 구름량 존재이유? ex:1시간 전에는 매우 흐렸고 1시간 평균도 높았는데, 지금은 맑아졌다. 날씨가 급격히 좋아지고 있다.\"\n",
    "df['temp_a_lag_12'] = g['temp_a'].shift(12) #1시간 전 기온 존재이유? ex:1시간 전에는 매우 더웠고 1시간 평균도 높았는데, 지금은 시원해졌다. 날씨가 급격히 좋아지고 있다.\"\n",
    "# 변동성(std) 특성 추가\n",
    "df['cloud_a_rolling_12_std'] = g['cloud_a'].shift(1).rolling(12, min_periods=1).std() #이건 표준편차\n",
    "df['uv_idx_rolling_12_std'] = g['uv_idx'].shift(1).rolling(12, min_periods=1).std()\n",
    "df['pressure_rolling_12_std'] = g['pressure'].shift(1).rolling(12, min_periods=1).std()\n",
    "df['temp_a_rolling_12_std'] = g['temp_a'].shift(1).rolling(12, min_periods=1).std()\n",
    "\n",
    "#차이\n",
    "df['is_precipitating'] = ((df['precip_1h'] > 0) | (df['rain'] > 0) | (df['snow'] > 0)).astype(int)#강수여부\n",
    "df['station_temp_diff'] = df['temp_b'] - df['temp_a']#기온차이\n",
    "df['station_cloud_diff'] = df['cloud_a'] - df['cloud_b']\n",
    "df['station_wind_spd_diff'] = df['wind_spd_a'] - df['wind_spd_b']\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "#--- 3. 물리 기반 및 순환 특성 생성 ---\n",
    "print(\"물리 기반 및 순환 특성 생성 중...\")\n",
    "\n",
    "df['temp_a_sq'] = df['temp_a']**2\n",
    "df['uv_idx_sq'] = df['uv_idx']**2#2계절관련, 너무덥거나 추울때 떨어지는 효율 반영\n",
    "df['sun_exposure_factor'] = df['real_feel_temp'].values - df['real_feel_temp_shade'].values#포화부족temp_a, dew_point는 제거고려\n",
    "df['diurnal_temp_range'] = df['temp_max'].values - df['temp_min'].values#일교차, temp_max, temp_min는 제거 고려\n",
    "df['saturation_deficit'] = df['temp_a'].values - df['dew_point'].values # 햇빛노출효과, real_feel_temp, real_feel_temp_shade는 제거 고려\n",
    "df['is_foggy'] = (df['vis'] < 1).astype(int) # 가시거리가 1km 미만이면 1 (안개)\n",
    "\n",
    "# 풍향(Wind Direction)의 순환성 변환 \n",
    "print(\"풍향(Wind Direction) 순환 특성 생성 중...\")#wind_dir_a와 wind_dir_b는 제거 1도와 361도를 다르게 만들가능성있음\n",
    "df['wind_dir_a_sin'] = np.sin(2 * np.pi * df['wind_dir_a'] / 360.0)\n",
    "df['wind_dir_a_cos'] = np.cos(2 * np.pi * df['wind_dir_a'] / 360.0)\n",
    "df['wind_dir_b_sin'] = np.sin(2 * np.pi * df['wind_dir_b'] / 360.0)\n",
    "df['wind_dir_b_cos'] = np.cos(2 * np.pi * df['wind_dir_b'] / 360.0)\n",
    "df['wind_power_a'] = df['wind_spd_a'] ** 3#풍향세기\n",
    "df['wind_power_b'] = df['wind_spd_b'] ** 3\n",
    "# 2. 방향별 바람의 세기 (예: \"북/남풍\"의 세기, \"동/서풍\"의 세기)\n",
    "df['wind_force_a_cos'] = df['wind_spd_a'] * df['wind_dir_a_cos']\n",
    "df['wind_force_a_sin'] = df['wind_spd_a'] * df['wind_dir_a_sin']\n",
    "df['wind_force_b_cos'] = df['wind_spd_b'] * df['wind_dir_b_cos']\n",
    "df['wind_force_b_sin'] = df['wind_spd_b'] * df['wind_dir_b_sin']\n",
    "\n",
    "# 시간-날씨 상호작용 특성 추가\n",
    "df['temp_x_hour'] = df['temp_a'] * df['hour_cos']\n",
    "df['cloud_x_hour'] = df['cloud_a'] * df['hour_cos']\n",
    "df['uv_idx_x_hour'] = df['uv_idx'] * df['hour_cos']\n",
    "df['humidity_x_hour'] = df['humidity'] * df['hour_cos']#중복되는 정보는?\n",
    "gc.collect()#메모리 회수\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"최종 결측치 처리 중...\")\n",
    "\n",
    "# 1. Lag/Rolling/Std 특성은 0으로 먼저 채웁니다., lag관련은 다 포함시킨거임 이게\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"최종 결측치 처리 중...\")\n",
    "\n",
    "# 1. Lag/Rolling/Std 특성은 0으로 먼저 채웁니다.\n",
    "lag_rolling_cols = [col for col in df.columns if '_lag_' in col or '_rolling_' in col]\n",
    "df[lag_rolling_cols] = df[lag_rolling_cols].fillna(0)\n",
    "\n",
    "# 2. 0/1 플래그(Flag) 변수들은 0으로 먼저 채웁니다.\n",
    "if 'is_precipitating' in df.columns:\n",
    "     df['is_precipitating'] = df['is_precipitating'].fillna(0)\n",
    "if 'is_foggy' in df.columns: \n",
    "     df['is_foggy'] = df['is_foggy'].fillna(0)\n",
    "\n",
    "# 3. ★★★ (수정) '화이트리스트' 방식: 보간할 '원본 센서 값'만 명시 ★★★\n",
    "# (가공된 특징(_sq, _sin, _anomaly 등)은 이 리스트에 없으므로, 보간(interpolate)되지 않습니다)\n",
    "cols_for_interpolation = [\n",
    "    'appr_temp', 'ceiling', 'cloud_b', 'dew_point', 'precip_1h', 'pressure',\n",
    "    'temp_b', 'uv_idx', 'vis', 'wind_dir_b', 'wind_spd_b',\n",
    "    'cloud_a', 'humidity', 'rain', 'snow', 'temp_a', 'wind_dir_a', 'wind_spd_a',\n",
    "    'real_feel_temp_shade', 'temp_max', 'temp_min', 'real_feel_temp'\n",
    "]\n",
    "# (혹시 파일 생성 시 FEATURES에서 제외했을 경우를 대비해, df에 존재하는 열만 선택)\n",
    "cols_for_interpolation = [col for col in cols_for_interpolation if col in df.columns]\n",
    "\n",
    "# 4. '연속형' 특성에 대해서만 발전소별 보간법 실행 (데이터 오염 방지)\n",
    "print(f\"다음 {len(cols_for_interpolation)}개 열(순수 날씨 변수)에 대해 ★발전소별 선형★ 보간을 수행합니다.\")\n",
    "df[cols_for_interpolation] = df.groupby('pv_id')[cols_for_interpolation].transform(lambda x: x.interpolate(method='linear', limit_direction='both'))\n",
    "\n",
    "# 5. 보간 후 남은 모든 NaN은 0으로 최종 처리 (안전장치)\n",
    "# (보간 대상이 아니었던 _anomaly, _sin, _sq 등의 NaN 값들이 여기서 0으로 처리됨)\n",
    "df.fillna(0, inplace=True)\n",
    "gc.collect()\n",
    "\n",
    "# 6. float32 변환 (메모리 최적화)\n",
    "print(\"모든 float64 칼럼을 float32로 강제 변환하여 메모리 사용량을 절반으로 줄입니다...\")\n",
    "# (select_dtypes 대신, df.columns를 순회하며 float64를 찾습니다 - 32비트 오류 방지)\n",
    "float64_cols = [col for col in df.columns if df[col].dtype == 'float64']\n",
    "for col in float64_cols:\n",
    "    df[col] = df[col].astype('float32')\n",
    "\n",
    "print(f\"{len(float64_cols)}개 칼럼을 float32로 변환 완료.\")\n",
    "gc.collect()\n",
    "# =============================================================================\n",
    "# Parquet 저장을 위해 pyarrow 라이브러리가 필요 (pip install pyarrow)#import할 필요는없음 백엔드엔진이라\n",
    "print(\"중간 처리된 데이터를 'processed_train.feather' 파일로 저장합니다...\")\n",
    "df.to_feather('processed_train.feather')#10분에 통합 청크더 올릴까 이제 메모리도 충분한데\n",
    "print(\"저장 완료.\")#82r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f03f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19236948, 83)\n",
      "   appr_temp  ceiling    cloud_b  dew_point  precip_1h  pressure     temp_b  uv_idx        vis  wind_dir_b  wind_spd_b   cloud_a   humidity  rain  snow     temp_a  wind_dir_a  wind_spd_a    coord1    coord2                      time    pv_id  real_feel_temp_shade   temp_max  temp_min  real_feel_temp  nins location_cluster  minute  hour  decimal_hour  day_of_year  hour_sin  hour_cos  day_of_year_sin  day_of_year_cos  seasonal_hour  ...  humidity_anomaly  uv_idx_rolling_12  uv_idx_lag_12  humidity_rolling_12  humidity_lag_12  pressure_rolling_12  pressure_lag_12  cloud_a_rolling_12  temp_a_rolling_12  cloud_a_lag_12  temp_a_lag_12  cloud_a_rolling_12_std  uv_idx_rolling_12_std  pressure_rolling_12_std  temp_a_rolling_12_std  is_precipitating  station_temp_diff  temp_a_sq  uv_idx_sq  sun_exposure_factor  diurnal_temp_range  saturation_deficit  is_foggy  wind_dir_a_sin  wind_dir_a_cos  wind_dir_b_sin  wind_dir_b_cos  wind_power_a  wind_power_b  wind_force_a_cos  wind_force_a_sin  \\\n",
      "0       28.9  12192.0  46.525425  24.710169        0.0    1006.0  26.003389     0.0  20.737288       180.0    0.386441  8.952276  88.042786   0.0   0.0  27.842995  178.187057     1.98989 -2.018131 -0.172021 2024-08-01 00:05:00+09:00  PV_ID_0              32.31356  27.882139     27.83        32.31356   0.0                6       5     0      0.083333          214  0.021815  0.999762        -0.513901        -0.857849      -0.011211  ...               0.0                0.0            0.0                  0.0              0.0                  0.0              0.0                 0.0                0.0             0.0            0.0                     0.0                    0.0                      0.0                    0.0                 0          -1.839605        0.0        0.0                  0.0                 0.0                 0.0         0             0.0             0.0             0.0             0.0           0.0           0.0               0.0               0.0   \n",
      "1       28.9  12192.0  46.525425  24.710169        0.0    1006.0  26.003389     0.0  20.737288       180.0    0.386441  8.952276  88.042786   0.0   0.0  27.842995  178.187057     1.98989 -2.018131 -0.172021 2024-08-01 00:10:00+09:00  PV_ID_0              32.31356  27.882139     27.83        32.31356   0.0                6      10     0      0.166667          214  0.043619  0.999048        -0.513901        -0.857849      -0.022416  ...               0.0                0.0            0.0                  0.0              0.0                  0.0              0.0                 0.0                0.0             0.0            0.0                     0.0                    0.0                      0.0                    0.0                 0          -1.839605        0.0        0.0                  0.0                 0.0                 0.0         0             0.0             0.0             0.0             0.0           0.0           0.0               0.0               0.0   \n",
      "2       28.9  12192.0  46.525425  24.710169        0.0    1006.0  26.003389     0.0  20.737288       180.0    0.386441  8.952276  88.042786   0.0   0.0  27.842995  178.187057     1.98989 -2.018131 -0.172021 2024-08-01 00:15:00+09:00  PV_ID_0              32.31356  27.882139     27.83        32.31356   0.0                6      15     0      0.250000          214  0.065403  0.997859        -0.513901        -0.857849      -0.033611  ...               0.0                0.0            0.0                  0.0              0.0                  0.0              0.0                 0.0                0.0             0.0            0.0                     0.0                    0.0                      0.0                    0.0                 0          -1.839605        0.0        0.0                  0.0                 0.0                 0.0         0             0.0             0.0             0.0             0.0           0.0           0.0               0.0               0.0   \n",
      "3       28.9  12192.0  46.525425  24.710169        0.0    1006.0  26.003389     0.0  20.737288       180.0    0.386441  8.952276  88.042786   0.0   0.0  27.842995  178.187057     1.98989 -2.018131 -0.172021 2024-08-01 00:20:00+09:00  PV_ID_0              32.31356  27.882139     27.83        32.31356   0.0                6      20     0      0.333333          214  0.087156  0.996195        -0.513901        -0.857849      -0.044789  ...               0.0                0.0            0.0                  0.0              0.0                  0.0              0.0                 0.0                0.0             0.0            0.0                     0.0                    0.0                      0.0                    0.0                 0          -1.839605        0.0        0.0                  0.0                 0.0                 0.0         0             0.0             0.0             0.0             0.0           0.0           0.0               0.0               0.0   \n",
      "4       28.9  12192.0  46.525425  24.710169        0.0    1006.0  26.003389     0.0  20.737288       180.0    0.386441  8.952276  88.042786   0.0   0.0  27.842995  178.187057     1.98989 -2.018131 -0.172021 2024-08-01 00:25:00+09:00  PV_ID_0              32.31356  27.882139     27.83        32.31356   0.0                6      25     0      0.416667          214  0.108867  0.994056        -0.513901        -0.857849      -0.055947  ...               0.0                0.0            0.0                  0.0              0.0                  0.0              0.0                 0.0                0.0             0.0            0.0                     0.0                    0.0                      0.0                    0.0                 0          -1.839605        0.0        0.0                  0.0                 0.0                 0.0         0             0.0             0.0             0.0             0.0           0.0           0.0               0.0               0.0   \n",
      "\n",
      "   wind_force_b_cos  wind_force_b_sin  temp_x_hour  cloud_x_hour  uv_idx_x_hour  humidity_x_hour  \n",
      "0               0.0     -3.378372e-08          0.0           0.0            0.0              0.0  \n",
      "1               0.0     -3.378372e-08          0.0           0.0            0.0              0.0  \n",
      "2               0.0     -3.378372e-08          0.0           0.0            0.0              0.0  \n",
      "3               0.0     -3.378372e-08          0.0           0.0            0.0              0.0  \n",
      "4               0.0     -3.378372e-08          0.0           0.0            0.0              0.0  \n",
      "\n",
      "[5 rows x 83 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_feather('processed_train.feather')\n",
    "print(df.shape)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', 75)\n",
    "pd.set_option('display.width', 1000)\n",
    "print(df.head())##현재 위도경도 포함됨 6추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25fcc630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow가 설치되었으며 버전은: 21.0.0\n",
      "pvlib가 설치되었으며 버전은: 0.13.1\n",
      "pandas가 설치되었으며 버전은: 2.3.2\n",
      "lightgbm이 설치되었으며 버전은: 4.6.0\n",
      "numpy가 설치되었으며 버전은: 2.3.3\n",
      "dask가 설치되었으며 버전은: 2025.9.1\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import lightgbm as lgb# lgb명칭으로 lightgbm외부라이브워리 불러옴\n",
    "import pvlib#태양광 발전 전문 태양 위치 등\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask as dd\n",
    "\n",
    "print(\"pyarrow가 설치되었으며 버전은:\", pa.__version__)\n",
    "print(\"pvlib가 설치되었으며 버전은:\", pvlib.__version__)\n",
    "print(\"pandas가 설치되었으며 버전은:\", pd.__version__)\n",
    "print(\"lightgbm이 설치되었으며 버전은:\", lgb.__version__)\n",
    "print(\"numpy가 설치되었으며 버전은:\", np.__version__)\n",
    "print(\"dask가 설치되었으며 버전은:\", dd.__version__)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
