{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49fa739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'processed_train.feather' 파일을 불러옵니다...\n",
      "데이터 로딩 완료.\n",
      "183개 발전소 전체 데이터를 학습용으로 준비합니다...\n",
      "개인화된 '일출/일몰 맵'을 기반으로 밤 시간대 데이터를 '학습에서 제외'합니다 (개선7 적용)...\n",
      "원본 데이터 19236948 행에서 '밤' 시간대 5888904 행을 제외합니다.\n",
      "시간 기반으로 '낮' 훈련/검증 세트를 분리합니다 (개선5 적용)...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.67 GiB for an array with shape (94, 13348044) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 135\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# '낮' 시간 데이터만 남김 (X_train, y_train 모두 동일한 마스크 적용)\u001b[39;00m\n\u001b[32m    128\u001b[39m \n\u001b[32m    129\u001b[39m \n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m#-------------------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# ★★★ (개선5) 시간 기반 분리 적용 (이미 '낮' 데이터만 남은 상태에서 수행) \u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m시간 기반으로 \u001b[39m\u001b[33m'\u001b[39m\u001b[33m낮\u001b[39m\u001b[33m'\u001b[39m\u001b[33m 훈련/검증 세트를 분리합니다 (개선5 적용)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m X_train = \u001b[43mX_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_day_mask\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    136\u001b[39m y_train = y_train.loc[train_day_mask]\n\u001b[32m    137\u001b[39m pv_id_helper = pv_id_helper.loc[train_day_mask]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1189\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1190\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexing.py:1413\u001b[39m, in \u001b[36m_LocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1411\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_slice_axis(key, axis=axis)\n\u001b[32m   1412\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m com.is_bool_indexer(key):\n\u001b[32m-> \u001b[39m\u001b[32m1413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getbool_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1414\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[32m   1415\u001b[39m     \u001b[38;5;66;03m# an iterable multi-selection\u001b[39;00m\n\u001b[32m   1416\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(labels, MultiIndex)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexing.py:1211\u001b[39m, in \u001b[36m_LocationIndexer._getbool_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1209\u001b[39m key = check_bool_indexer(labels, key)\n\u001b[32m   1210\u001b[39m inds = key.nonzero()[\u001b[32m0\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m1211\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:4172\u001b[39m, in \u001b[36mNDFrame._take_with_is_copy\u001b[39m\u001b[34m(self, indices, axis)\u001b[39m\n\u001b[32m   4161\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   4162\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis = \u001b[32m0\u001b[39m) -> Self:\n\u001b[32m   4163\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4164\u001b[39m \u001b[33;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[32m   4165\u001b[39m \u001b[33;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4170\u001b[39m \u001b[33;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[32m   4171\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4172\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4173\u001b[39m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[32m   4174\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result._get_axis(axis).equals(\u001b[38;5;28mself\u001b[39m._get_axis(axis)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:4152\u001b[39m, in \u001b[36mNDFrame.take\u001b[39m\u001b[34m(self, indices, axis, **kwargs)\u001b[39m\n\u001b[32m   4147\u001b[39m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[32m   4148\u001b[39m     indices = np.arange(\n\u001b[32m   4149\u001b[39m         indices.start, indices.stop, indices.step, dtype=np.intp\n\u001b[32m   4150\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4152\u001b[39m new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4154\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes).__finalize__(\n\u001b[32m   4158\u001b[39m     \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mtake\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4159\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[39m, in \u001b[36mBaseBlockManager.take\u001b[39m\u001b[34m(self, indexer, axis, verify)\u001b[39m\n\u001b[32m    891\u001b[39m indexer = maybe_convert_indices(indexer, n, verify=verify)\n\u001b[32m    893\u001b[39m new_labels = \u001b[38;5;28mself\u001b[39m.axes[axis].take(indexer)\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:688\u001b[39m, in \u001b[36mBaseBlockManager.reindex_indexer\u001b[39m\u001b[34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[39m\n\u001b[32m    680\u001b[39m     new_blocks = \u001b[38;5;28mself\u001b[39m._slice_take_blocks_ax0(\n\u001b[32m    681\u001b[39m         indexer,\n\u001b[32m    682\u001b[39m         fill_value=fill_value,\n\u001b[32m    683\u001b[39m         only_slice=only_slice,\n\u001b[32m    684\u001b[39m         use_na_proxy=use_na_proxy,\n\u001b[32m    685\u001b[39m     )\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    687\u001b[39m     new_blocks = [\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m         \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    695\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks\n\u001b[32m    696\u001b[39m     ]\n\u001b[32m    698\u001b[39m new_axes = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.axes)\n\u001b[32m    699\u001b[39m new_axes[axis] = new_axis\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1373\u001b[39m, in \u001b[36mBlock.take_nd\u001b[39m\u001b[34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[39m\n\u001b[32m   1370\u001b[39m     allow_fill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1372\u001b[39m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1373\u001b[39m new_values = \u001b[43malgos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[32m   1378\u001b[39m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[32m   1380\u001b[39m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[32m   1381\u001b[39m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[39m, in \u001b[36mtake_nd\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n\u001b[32m    116\u001b[39m arr = np.asarray(arr)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wch23\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:157\u001b[39m, in \u001b[36m_take_nd_ndarray\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    155\u001b[39m     out = np.empty(out_shape, dtype=dtype, order=\u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     out = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m func = _get_take_nd_function(\n\u001b[32m    160\u001b[39m     arr.ndim, arr.dtype, out.dtype, axis=axis, mask_info=mask_info\n\u001b[32m    161\u001b[39m )\n\u001b[32m    162\u001b[39m func(arr, indexer, out, fill_value)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 4.67 GiB for an array with shape (94, 13348044) and data type float32"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# [스크립트 1] 최종 모델 훈련 (개선5: 시간 기반 검증, 개선7: 밤 데이터 제거 적용)\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import warnings\n",
    "import gc\n",
    "import os\n",
    "import joblib \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "BASE_PROCESSED_FILE = 'processed_train.feather'\n",
    "\n",
    "# --- 1. 최종 전처리된 데이터 로딩 ---\n",
    "print(f\"'{BASE_PROCESSED_FILE}' 파일을 불러옵니다...\")\n",
    "df = pd.read_feather(BASE_PROCESSED_FILE)\n",
    "print(\"데이터 로딩 완료.\")\n",
    "\n",
    "# --- 2. 데이터 분리 (제거) ---\n",
    "print(\"183개 발전소 전체 데이터를 학습용으로 준비합니다...\")\n",
    "\n",
    "# --- 3. 최종 학습 데이터 준비 ---\n",
    "features = [\n",
    "    #'appr_temp', #'dew_point', \n",
    "    'ceiling', 'cloud_b', \n",
    "    'precip_1h', 'pressure',\n",
    "    'temp_b', 'uv_idx',\n",
    "    'vis', #'wind_dir_b',  \n",
    "    'wind_spd_b',#UV때문에 더 좋아짐\n",
    "    'cloud_a', 'humidity', 'rain', 'snow',\n",
    "    'wind_spd_a',#'pv_id',                                                \n",
    "    #'coord1', 'coord2', ,#'temp_max', 'temp_min', #'wind_dir_a', #'real_feel_temp', 'real_feel_temp_shade', \n",
    "    # 이미 다 압축반영, 좌표는 location이 다 가지고있고ㅇㅇ\n",
    "    #15개  11개 제외+ 처음부터 7개제외 원본\n",
    "\n",
    "   #날씨관련\n",
    "    'is_precipitating','station_temp_diff',\n",
    "    'cloud_x_hour','uv_idx_x_hour','humidity_x_hour','temp_x_hour','uv_idx_sq','temp_a_sq','station_wind_spd_diff','station_cloud_diff',\n",
    "    'is_foggy',#가시거리까지23\n",
    "   #lag날씨\n",
    "# (새로운 15분, 30분, 1시간 Lag/Rolling/Std 특성)\n",
    "    'cloud_a_lag_3','cloud_a_rolling_3_mean','cloud_a_rolling_3_std',\n",
    "    'cloud_a_lag_6','cloud_a_rolling_6_mean','cloud_a_rolling_6_std',\n",
    "    'cloud_a_lag_12','cloud_a_rolling_12_mean','cloud_a_rolling_12_std',\n",
    "    'temp_a_lag_3','temp_a_rolling_3_mean','temp_a_rolling_3_std',\n",
    "    'temp_a_lag_6','temp_a_rolling_6_mean','temp_a_rolling_6_std',\n",
    "    'temp_a_lag_12','temp_a_rolling_12_mean','temp_a_rolling_12_std',\n",
    "    'uv_idx_lag_3','uv_idx_rolling_3_mean','uv_idx_rolling_3_std',\n",
    "    'uv_idx_lag_6','uv_idx_rolling_6_mean','uv_idx_rolling_6_std',\n",
    "    'uv_idx_lag_12','uv_idx_rolling_12_mean','uv_idx_rolling_12_std',\n",
    "    'pressure_lag_3','pressure_rolling_3_mean','pressure_rolling_3_std',\n",
    "    'pressure_lag_6','pressure_rolling_6_mean','pressure_rolling_6_std',\n",
    "    'pressure_lag_12','pressure_rolling_12_mean','pressure_rolling_12_std',\n",
    "    'humidity_lag_3','humidity_rolling_3_mean','humidity_rolling_3_std',\n",
    "    'humidity_lag_6','humidity_rolling_6_mean','humidity_rolling_6_std',\n",
    "    'humidity_lag_12','humidity_rolling_12_mean','humidity_rolling_12_std',\n",
    "\n",
    "    #군집\n",
    "\n",
    "    'location_cluster','climate_cluster',#2\n",
    "\n",
    "    #물리기반순환\n",
    "    'seasonal_hour',\n",
    "    'sun_exposure_factor', 'diurnal_temp_range',\n",
    "    'saturation_deficit',\n",
    "    'hour_sin', 'hour_cos',#'decimal_hour',\n",
    "    'day_of_year_sin', 'day_of_year_cos',\n",
    "    'hour', #min,'day_of_year'제외 #11개\n",
    "    'daylight_cosine',\n",
    "\n",
    "    #풍향\n",
    "    'wind_power_a','wind_power_b',#'temp_a_sq','uv_idx_sq',\n",
    "    #6\n",
    "\n",
    "    #일몰 일출 특성변수\n",
    "    'sunset_decimal','sunrise_decimal',\n",
    "    \n",
    "    'cloud_anomaly','uv_anomaly','temp_anomaly','humidity_anomaly','vis_anomaly','wind_anomaly','pressure_anomaly',\n",
    "    'cluster_hour_humidity_mean','cluster_hour_temp_mean','cluster_hour_uv_mean','cluster_hour_cloud_mean','cluster_hour_vis_mean',\n",
    "    'cluster_hour_wind_mean','cluster_hour_pressure_mean'\n",
    "]\n",
    "target = 'nins'\n",
    "\n",
    "# 183개 전체 데이터를 X_train, y_train으로 정의\n",
    "X_train = df[features]\n",
    "y_train = df[target]\n",
    "\n",
    "# df에서 헬퍼 컬럼을 가져옴\n",
    "decimal_hours = df['decimal_hour'].to_numpy()\n",
    "sunrise_times = df['sunrise_decimal'].to_numpy()\n",
    "sunset_times = df['sunset_decimal'].to_numpy()#이게 테스트파일생성코드에서 맵을 이용해 정의한것? 아 애초에 train파일에 있던 값이네\n",
    "pv_id_helper = df['pv_id']#목록에서 pvid제거시\n",
    "\n",
    "#if 'pv_id' in X_train.columns:\n",
    "#    X_train['pv_id'] = X_train['pv_id'].astype('category')\n",
    "if 'location_cluster' in X_train.columns:\n",
    "    X_train['location_cluster'] = X_train['location_cluster'].astype('category')\n",
    "if 'climate_cluster' in X_train.columns:\n",
    "    X_train['climate_cluster'] = X_train['climate_cluster'].astype('category')\n",
    "\n",
    "print(\"개인화된 '일출/일몰 맵'을 기반으로 밤 시간대 데이터를 '학습에서 제외'합니다 (개선7 적용)...\")\n",
    "gc.collect()\n",
    "\n",
    "# 기본값 설정\n",
    "default_sunrise = 4.5 \n",
    "default_sunset = 20.5 \n",
    "sunrise_times[sunrise_times < -900] = default_sunrise #NUN값들 즉 병합실패한 값들을 기본값으로 설정해줌\n",
    "sunset_times[sunset_times < -900] = default_sunset # 군집별 일출, 일몰시간임\n",
    "sunrise_times[sunrise_times == 0] = default_sunrise#마찬가지 물리적으로 저 시간대가 0일리는 없으니 오류겠지하는거임\n",
    "sunset_times[sunset_times == 0] = default_sunset\n",
    "\n",
    "# '밤' 시간 마스크 생성\n",
    "train_morning_mask = (decimal_hours <= sunrise_times)#일출시간보다 작음 즉 0이겠지\n",
    "train_evening_mask = (decimal_hours >= sunset_times)#일몰후의 시간 즉 0이겠지\n",
    "train_night_mask = train_morning_mask | train_evening_mask#아침또는 밤을 만족하는 즉 위의 두값에 포함되는 시간대는 night다 0이다\n",
    "\n",
    "\n",
    "# [개선7 적용] '밤' 시간 데이터 제거 이미 test에 적용이 됐으니 그리고 위의 과정으로 확실히 했으니 굳이 복잡하게 학습 할 필요는 없어서 제\n",
    "# (O) 개선 방식: '밤'이 아닌 '낮' 시간 데이터만 선택\n",
    "\n",
    "train_day_mask = ~train_night_mask # '밤' 마스크의 반대(~) = '낮'\n",
    "\n",
    "print(f\"원본 데이터 {len(X_train)} 행에서 '밤' 시간대 {np.sum(train_night_mask)} 행을 제외합니다.\")\n",
    "\n",
    "# '낮' 시간 데이터만 남김 (X_train, y_train 모두 동일한 마스크 적용)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "# ★★★ (개선5) 시간 기반 분리 적용 (이미 '낮' 데이터만 남은 상태에서 수행) \n",
    "\n",
    "print(\"시간 기반으로 '낮' 훈련/검증 세트를 분리합니다 (개선5 적용)...\")\n",
    "\n",
    "X_train = X_train.loc[train_day_mask]\n",
    "y_train = y_train.loc[train_day_mask]\n",
    "pv_id_helper = pv_id_helper.loc[train_day_mask]\n",
    "\n",
    "\n",
    "print(f\" -> '낮' 시간 학습 데이터: {len(X_train)} 행\")\n",
    "\n",
    "# 원본 df 메모리 정리\n",
    "del df, decimal_hours, sunrise_times, sunset_times, train_night_mask, train_day_mask\n",
    "# (time_helper도 여기서 del 됩니다)\n",
    "gc.collect()\n",
    "\n",
    "# [오류 수정] 'pv_id' 기반 분리 (하드코딩 대신 실제 ID 사용)\n",
    "print(\"발전소 ID 기반으로 훈련/검증 세트를 분리합니다...\")\n",
    "\n",
    "# 1. '낮' 데이터에 존재하는 *모든* 고유 pv_id 목록을 가져옵니다.\n",
    "\n",
    "VALIDATION_PV_IDS = [ \n",
    "    138, 139, 107, 14, 16, 164, 167, 169, 170, 193,\n",
    "    22, 28, 32, 45, 51, 56, 59, 77, 80, 86,9\n",
    "] \n",
    "\n",
    "valid_idx_mask = pv_id_helper.isin(VALIDATION_PV_IDS)#수동으로 정한 리스트에 포함됐는지 확인\n",
    "train_idx_mask = ~valid_idx_mask # (valid의 반대 = train)\n",
    "\n",
    "print(f\"검증용 발전소 ({len(VALIDATION_PV_IDS)}개 수동 지정): {VALIDATION_PV_IDS}\")\n",
    "# 3. 'pv_id' 컬럼을 사용하여 인덱스 마스크 생성\n",
    "\n",
    "# 4. .loc를 사용하여 분리\n",
    "X_train_final = X_train.loc[train_idx_mask]# 특정 시간대만 가져오기 위한것 위에서 만들었던거 이용\n",
    "y_train_final = y_train.loc[train_idx_mask]\n",
    "\n",
    "X_valid_final = X_train.loc[valid_idx_mask]\n",
    "y_valid_final = y_train.loc[valid_idx_mask]\n",
    "\n",
    "# 5. (안전 장치) 분리 후 검증 세트가 비어있는지 다시 확인\n",
    "if X_valid_final.empty:\n",
    "\n",
    "    exit\n",
    "else:\n",
    "    print(f\"   -> 훈련: {len(X_train_final)} 행, 검증: {len(X_valid_final)} 행\")\n",
    "\n",
    "# (메모리 정리)\n",
    "# (time_helper는 이미 del 되었으므로 리스트에서 제외)\n",
    "del X_train, y_train, train_idx_mask, valid_idx_mask\n",
    "gc.collect()\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "# (이하 final_model.fit(...) 코드는 동일)\n",
    "# ...\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- 5. '기본 파라미터'로 모델 생성 ---\n",
    "final_model = lgb.LGBMRegressor(\n",
    "    objective='mae',\n",
    "    metric='mae',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    n_estimators=3000, \n",
    ")\n",
    "\n",
    "# --- 6. '검증 세트'로 학습 (★수정됨★) ---\n",
    "print(\"\\n'훈련 데이터'로 학습하고 '검증 데이터'로 조기 종료를 시작합니다...\")\n",
    "final_model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    eval_set=[(X_valid_final, y_valid_final)],\n",
    "    callbacks=[lgb.early_stopping(350, verbose=True)],\n",
    "    categorical_feature=['location_cluster','climate_cluster']#pvid제거\n",
    ")\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# 훈련된 모델의 '특성 중요도' 확인\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"모델 훈련 완료. '특성 중요도'를 분석합니다...\")\n",
    "\n",
    "# 1. 특성 이름(features)과 중요도 점수(importances)를 DataFrame으로 합칩니다.\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': final_model.feature_importances_\n",
    "})\n",
    "\n",
    "# 2. 중요도(importance)가 높은 순서대로 정렬합니다.\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"\\n[상위 50개 중요 특성]:\")\n",
    "print(importance_df.head(50))\n",
    "\n",
    "# 3. 중요도가 0인, 즉 모델이 '전혀 사용하지 않은' 특성을 찾습니다.\n",
    "zero_importance_features = importance_df[importance_df['importance'] == 0]\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"[중요도 0인 특성 (제거 후보): {len(zero_importance_features)}개]\")\n",
    "print(zero_importance_features['feature'].tolist())\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# ★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\n",
    "\n",
    "# --- 7. 최종 모델 저장 ---\n",
    "print(\"최종 모델을 'final_model.joblib'로 저장합니다...\")\n",
    "joblib.dump(final_model, 'final_model.joblib5')\n",
    "\n",
    "print(f\"학습에 사용된 {len(features)}개 특성 리스트를 'final_features.joblib'로 저장합니다...\")\n",
    "joblib.dump(features, 'final_features.joblib')\n",
    "\n",
    "print(\"저장 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c57ba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'processed_train.feather' 파일을 불러옵니다...\n",
      "데이터 로딩 완료.\n",
      "183개 발전소 전체 데이터를 학습용으로 준비합니다...\n",
      "개인화된 '일출/일몰 맵'을 기반으로 밤 시간대 데이터를 '학습에서 제외'합니다...\n",
      " -> '낮' 시간 학습 데이터: 13348044 행\n",
      "발전소 ID 기반으로 훈련/검증 세트를 분리합니다...\n",
      "검증용 발전소 (3개): ['PV_ID_97', 'PV_ID_98', 'PV_ID_99']\n",
      "Categories (183, object): ['PV_ID_0', 'PV_ID_1', 'PV_ID_100', 'PV_ID_101', ..., 'PV_ID_96', 'PV_ID_97', 'PV_ID_98', 'PV_ID_99']\n",
      "   -> 훈련: 13135614 행, 검증: 212430 행\n",
      "\n",
      "'훈련 데이터'로 학습하고 '검증 데이터'로 조기 종료를 시작합니다...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.996915 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 23365\n",
      "[LightGBM] [Info] Number of data points in the train set: 13135614, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 129.100006\n",
      "Training until validation scores don't improve for 350 rounds\n",
      "Early stopping, best iteration is:\n",
      "[224]\tvalid_0's l1: 72.3818\n",
      "\n",
      "==================================================\n",
      "모델 훈련 완료. '특성 중요도'를 분석합니다...\n",
      "\n",
      "[상위 50개 중요 특성]:\n",
      "                       feature  importance\n",
      "79             day_of_year_cos        1183\n",
      "76                    hour_sin         727\n",
      "13                       pv_id         621\n",
      "16                cloud_x_hour         540\n",
      "72               seasonal_hour         448\n",
      "50      uv_idx_rolling_12_mean         357\n",
      "95        cluster_hour_uv_mean         324\n",
      "78             day_of_year_sin         323\n",
      "94      cluster_hour_temp_mean         291\n",
      "77                    hour_cos         238\n",
      "49               uv_idx_lag_12         222\n",
      "73         sun_exposure_factor         207\n",
      "19                 temp_x_hour         132\n",
      "46                uv_idx_lag_6         122\n",
      "18             humidity_x_hour         122\n",
      "17               uv_idx_x_hour          75\n",
      "1                      cloud_b          72\n",
      "10                        rain          71\n",
      "87                  uv_anomaly          67\n",
      "0                      ceiling          56\n",
      "70            location_cluster          44\n",
      "8                      cloud_a          38\n",
      "81             daylight_cosine          36\n",
      "2                    precip_1h          32\n",
      "9                     humidity          31\n",
      "89            humidity_anomaly          25\n",
      "44       uv_idx_rolling_3_mean          23\n",
      "4                       temp_b          23\n",
      "75          saturation_deficit          23\n",
      "98      cluster_hour_wind_mean          22\n",
      "43                uv_idx_lag_3          19\n",
      "88                temp_anomaly          18\n",
      "40               temp_a_lag_12          17\n",
      "51       uv_idx_rolling_12_std          17\n",
      "5                       uv_idx          15\n",
      "96     cluster_hour_cloud_mean          12\n",
      "86               cloud_anomaly          11\n",
      "14            is_precipitating          10\n",
      "80                        hour           8\n",
      "32     cloud_a_rolling_12_mean           8\n",
      "22       station_wind_spd_diff           8\n",
      "61              humidity_lag_3           7\n",
      "6                          vis           7\n",
      "23          station_cloud_diff           6\n",
      "42       temp_a_rolling_12_std           6\n",
      "93  cluster_hour_humidity_mean           6\n",
      "62     humidity_rolling_3_mean           4\n",
      "48        uv_idx_rolling_6_std           4\n",
      "90                 vis_anomaly           3\n",
      "33      cloud_a_rolling_12_std           3\n",
      "\n",
      "==================================================\n",
      "[중요도 0인 특성 (제거 후보): 27개]\n",
      "['snow', 'uv_idx_sq', 'cloud_a_rolling_3_std', 'cloud_a_rolling_6_std', 'cloud_a_lag_12', 'is_foggy', 'uv_idx_rolling_3_std', 'temp_a_rolling_6_mean', 'temp_a_rolling_3_std', 'temp_a_rolling_6_std', 'pressure_rolling_12_mean', 'humidity_rolling_6_std', 'humidity_rolling_3_std', 'pressure_rolling_12_std', 'pressure_rolling_3_mean', 'pressure_rolling_3_std', 'pressure_rolling_6_mean', 'pressure_rolling_6_std', 'humidity_rolling_12_std', 'wind_power_b', 'wind_power_a', 'climate_cluster', 'diurnal_temp_range', 'sunrise_decimal', 'sunset_decimal', 'pressure_anomaly', 'cluster_hour_vis_mean']\n",
      "==================================================\n",
      "\n",
      "최종 모델을 'final_model.joblib'로 저장합니다...\n",
      "학습에 사용된 100개 특성 리스트를 'final_features.joblib'로 저장합니다...\n",
      "저장 완료.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# [스크립트 1] 최종 모델 훈련 (발전소 3개 검증 / 데이터 누수 위험 있음 앞의 발전소군집을 통해 뒤의 발전소 값을 엿보는)\n",
    "\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import warnings\n",
    "import gc\n",
    "import os\n",
    "import joblib \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "BASE_PROCESSED_FILE = 'processed_train.feather'\n",
    "\n",
    "# --- 1. 최종 전처리된 데이터 로딩 ---\n",
    "print(f\"'{BASE_PROCESSED_FILE}' 파일을 불러옵니다...\")\n",
    "df = pd.read_feather(BASE_PROCESSED_FILE)\n",
    "print(\"데이터 로딩 완료.\")\n",
    "\n",
    "# --- 2. 데이터 분리 (제거) ---\n",
    "print(\"183개 발전소 전체 데이터를 학습용으로 준비합니다...\")\n",
    "\n",
    "# --- 3. 최종 학습 데이터 준비 ---\n",
    "features = [\n",
    "    # (이전과 동일한 features 리스트...)\n",
    "    'ceiling', 'cloud_b', 'precip_1h', 'pressure', 'temp_b', 'uv_idx',\n",
    "    'vis', 'wind_spd_b', 'cloud_a', 'humidity', 'rain', 'snow',\n",
    "    'wind_spd_a', 'pv_id', 'is_precipitating', 'station_temp_diff',\n",
    "    'cloud_x_hour', 'uv_idx_x_hour', 'humidity_x_hour', 'temp_x_hour',\n",
    "    'uv_idx_sq', 'temp_a_sq', 'station_wind_spd_diff', 'station_cloud_diff', 'is_foggy',\n",
    "    'cloud_a_lag_3', 'cloud_a_rolling_3_mean', 'cloud_a_rolling_3_std',\n",
    "    'cloud_a_lag_6', 'cloud_a_rolling_6_mean', 'cloud_a_rolling_6_std',\n",
    "    'cloud_a_lag_12', 'cloud_a_rolling_12_mean', 'cloud_a_rolling_12_std',\n",
    "    'temp_a_lag_3', 'temp_a_rolling_3_mean', 'temp_a_rolling_3_std',\n",
    "    'temp_a_lag_6', 'temp_a_rolling_6_mean', 'temp_a_rolling_6_std',\n",
    "    'temp_a_lag_12', 'temp_a_rolling_12_mean', 'temp_a_rolling_12_std',\n",
    "    'uv_idx_lag_3', 'uv_idx_rolling_3_mean', 'uv_idx_rolling_3_std',\n",
    "    'uv_idx_lag_6', 'uv_idx_rolling_6_mean', 'uv_idx_rolling_6_std',\n",
    "    'uv_idx_lag_12', 'uv_idx_rolling_12_mean', 'uv_idx_rolling_12_std',\n",
    "    'pressure_lag_3', 'pressure_rolling_3_mean', 'pressure_rolling_3_std',\n",
    "    'pressure_lag_6', 'pressure_rolling_6_mean', 'pressure_rolling_6_std',\n",
    "    'pressure_lag_12', 'pressure_rolling_12_mean', 'pressure_rolling_12_std',\n",
    "    'humidity_lag_3', 'humidity_rolling_3_mean', 'humidity_rolling_3_std',\n",
    "    'humidity_lag_6', 'humidity_rolling_6_mean', 'humidity_rolling_6_std',\n",
    "    'humidity_lag_12', 'humidity_rolling_12_mean', 'humidity_rolling_12_std',\n",
    "    'location_cluster', 'climate_cluster', 'seasonal_hour',\n",
    "    'sun_exposure_factor', 'diurnal_temp_range', 'saturation_deficit',\n",
    "    'hour_sin', 'hour_cos', 'day_of_year_sin', 'day_of_year_cos', 'hour',\n",
    "    'daylight_cosine', 'wind_power_a', 'wind_power_b', 'sunset_decimal',\n",
    "    'sunrise_decimal', 'cloud_anomaly', 'uv_anomaly', 'temp_anomaly',\n",
    "    'humidity_anomaly', 'vis_anomaly', 'wind_anomaly', 'pressure_anomaly',\n",
    "    'cluster_hour_humidity_mean', 'cluster_hour_temp_mean',\n",
    "    'cluster_hour_uv_mean', 'cluster_hour_cloud_mean',\n",
    "    'cluster_hour_vis_mean', 'cluster_hour_wind_mean', 'cluster_hour_pressure_mean'\n",
    "]\n",
    "target = 'nins'\n",
    "\n",
    "# (안전장치) df에 없는 특성 자동 제외\n",
    "features = [f for f in features if f in df.columns]\n",
    "\n",
    "X_train = df[features]\n",
    "y_train = df[target]\n",
    "if 'pv_id' in X_train.columns:\n",
    "    X_train['pv_id'] = X_train['pv_id'].astype('category')\n",
    "if 'location_cluster' in X_train.columns:\n",
    "    X_train['location_cluster'] = X_train['location_cluster'].astype('category')\n",
    "if 'climate_cluster' in X_train.columns:\n",
    "    X_train['climate_cluster'] = X_train['climate_cluster'].astype('category')\n",
    "\n",
    "\n",
    "# ★★★ '밤 데이터 제거' 로직 (먼저 실행) ★★★\n",
    "print(\"개인화된 '일출/일몰 맵'을 기반으로 밤 시간대 데이터를 '학습에서 제외'합니다...\")\n",
    "decimal_hours = df['decimal_hour'].to_numpy()\n",
    "sunrise_times = df['sunrise_decimal'].to_numpy()\n",
    "sunset_times = df['sunset_decimal'].to_numpy()\n",
    "\n",
    "default_sunrise = 4.5 \n",
    "default_sunset = 20.5 \n",
    "sunrise_times[sunrise_times < -900] = default_sunrise \n",
    "sunset_times[sunset_times < -900] = default_sunset \n",
    "sunrise_times[sunrise_times == 0] = default_sunrise\n",
    "sunset_times[sunset_times == 0] = default_sunset\n",
    "\n",
    "train_morning_mask = (decimal_hours <= sunrise_times)\n",
    "train_evening_mask = (decimal_hours >= sunset_times)\n",
    "train_night_mask = train_morning_mask | train_evening_mask\n",
    "train_day_mask = ~train_night_mask # '낮' 데이터만 True\n",
    "\n",
    "# '낮' 시간 데이터만 남김 (X_train, y_train, time_helper 모두 동일한 마스크 적용)\n",
    "X_train = X_train.loc[train_day_mask]\n",
    "y_train = y_train.loc[train_day_mask]\n",
    "\n",
    "\n",
    "print(f\" -> '낮' 시간 학습 데이터: {len(X_train)} 행\")\n",
    "\n",
    "# 원본 df 메모리 정리\n",
    "del df, decimal_hours, sunrise_times, sunset_times, train_night_mask, train_day_mask\n",
    "# (time_helper도 여기서 del 됩니다)\n",
    "gc.collect()\n",
    "\n",
    "# ★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\n",
    "# [오류 수정] 'pv_id' 기반 분리 (하드코딩 대신 실제 ID 사용)\n",
    "# ★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\n",
    "\n",
    "print(\"발전소 ID 기반으로 훈련/검증 세트를 분리합니다...\")\n",
    "\n",
    "# 1. '낮' 데이터에 존재하는 *모든* 고유 pv_id 목록을 가져옵니다.\n",
    "all_pv_ids_in_day_data = X_train['pv_id'].unique()\n",
    "\n",
    "# 2. 이 목록에서 검증용으로 사용할 3개의 ID를 *자동으로* 선택합니다.\n",
    "# (예: 목록의 마지막 3개 ID. [0:3]으로 바꾸면 처음 3개 ID)\n",
    "VALIDATION_PV_IDS = all_pv_ids_in_day_data[-3:] \n",
    "print(f\"검증용 발전소 (3개): {VALIDATION_PV_IDS}\")\n",
    "\n",
    "# 3. 'pv_id' 컬럼을 사용하여 인덱스 마스크 생성\n",
    "valid_idx_mask = X_train['pv_id'].isin(VALIDATION_PV_IDS)\n",
    "train_idx_mask = ~valid_idx_mask # (valid의 반대 = train)\n",
    "\n",
    "# 4. .loc를 사용하여 분리\n",
    "X_train_final = X_train.loc[train_idx_mask]\n",
    "y_train_final = y_train.loc[train_idx_mask]\n",
    "\n",
    "X_valid_final = X_train.loc[valid_idx_mask]\n",
    "y_valid_final = y_train.loc[valid_idx_mask]\n",
    "\n",
    "# 5. (안전 장치) 분리 후 검증 세트가 비어있는지 다시 확인\n",
    "if X_valid_final.empty:\n",
    "    print(\"=\"*50)\n",
    "    print(\"!! 치명적 오류: 검증 세트(X_valid_final)가 비어있습니다.\")\n",
    "    print(\"   'VALIDATION_PV_IDS'에 문제가 없는지 확인하십시오.\")\n",
    "    print(\"=\"*50)\n",
    "    # (여기서 스크립트를 중지시키는 것이 좋지만, 일단 진행합니다.)\n",
    "else:\n",
    "    print(f\"   -> 훈련: {len(X_train_final)} 행, 검증: {len(X_valid_final)} 행\")\n",
    "\n",
    "# (메모리 정리)\n",
    "# (time_helper는 이미 del 되었으므로 리스트에서 제외)\n",
    "del X_train, y_train, train_idx_mask, valid_idx_mask\n",
    "gc.collect()\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "# (이하 final_model.fit(...) 코드는 동일)\n",
    "# ...\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# --- 5. '기본 파라미터'로 모델 생성 ---\n",
    "final_model = lgb.LGBMRegressor(\n",
    "    objective='mae',\n",
    "    metric='mae',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    n_estimators=3000, \n",
    ")\n",
    "\n",
    "# --- 6. '검증 세트'로 학습 (★수정됨★) ---\n",
    "print(\"\\n'훈련 데이터'로 학습하고 '검증 데이터'로 조기 종료를 시작합니다...\")\n",
    "final_model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    eval_set=[(X_valid_final, y_valid_final)],\n",
    "    callbacks=[lgb.early_stopping(350, verbose=True)],\n",
    "    categorical_feature=['pv_id','location_cluster','climate_cluster']\n",
    ")\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# 훈련된 모델의 '특성 중요도' 확인\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"모델 훈련 완료. '특성 중요도'를 분석합니다...\")\n",
    "\n",
    "# 1. 특성 이름(features)과 중요도 점수(importances)를 DataFrame으로 합칩니다.\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': final_model.feature_importances_\n",
    "})\n",
    "\n",
    "# 2. 중요도(importance)가 높은 순서대로 정렬합니다.\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"\\n[상위 50개 중요 특성]:\")\n",
    "print(importance_df.head(50))\n",
    "\n",
    "# 3. 중요도가 0인, 즉 모델이 '전혀 사용하지 않은' 특성을 찾습니다.\n",
    "zero_importance_features = importance_df[importance_df['importance'] == 0]\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"[중요도 0인 특성 (제거 후보): {len(zero_importance_features)}개]\")\n",
    "print(zero_importance_features['feature'].tolist())\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# ★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\n",
    "\n",
    "# --- 7. 최종 모델 저장 ---\n",
    "print(\"최종 모델을 'final_model.joblib'로 저장합니다...\")\n",
    "joblib.dump(final_model, 'final_model.joblib5')\n",
    "\n",
    "print(f\"학습에 사용된 {len(features)}개 특성 리스트를 'final_features.joblib'로 저장합니다...\")\n",
    "joblib.dump(features, 'final_features.joblib')\n",
    "\n",
    "print(\"저장 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f1e885a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련된 최종 모델 (final_model.joblib5)과 특성 리스트 (final_features.joblib)를 불러옵니다...\n",
      "-> 모델 로드 완료. 예측에 사용할 100개 특성 리스트 로드 완료.\n",
      "'processed_test.feather' 파일을 불러옵니다...\n",
      "최종 예측을 수행합니다...\n",
      "개인화된 '일출/일몰 맵'을 기반으로 밤 시간대 예측을 0으로 강제 조정합니다...\n",
      "동적 마스크 적용 완료.\n",
      "제출 파일을 생성합니다...\n",
      "==================================================\n",
      "최종 제출 파일 'submission.csv' 생성이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import warnings\n",
    "import gc\n",
    "import os\n",
    "import joblib \n",
    "\n",
    "# --- 1. 파일 경로 설정 ---\n",
    "# (모델과 특성 파일은 '모델저장파일코드'에서 생성된 파일과 이름이 같아야 합니다)\n",
    "MODEL_FILE = 'final_model.joblib5'          # ★ 사용하시는 모델 파일명\n",
    "FEATURES_FILE = 'final_features.joblib'   # ★ '모델저장파일코드'에서 저장한 특성 리스트 파일\n",
    "PROCESSED_TEST_FILE = 'processed_test.feather'   # 전처리된 테스트 파일\n",
    "SUBMISSION_TEMPLATE = 'submission_sample.csv'\n",
    "SUBMISSION_FILE = 'submission.csv'\n",
    "\n",
    "# --- 2. 모델 및 '특성 리스트' 로드 ---\n",
    "print(f\"훈련된 최종 모델 ({MODEL_FILE})과 특성 리스트 ({FEATURES_FILE})를 불러옵니다...\")\n",
    "final_model = joblib.load(MODEL_FILE)\n",
    "features = joblib.load(FEATURES_FILE)\n",
    "            \n",
    "print(f\"-> 모델 로드 완료. 예측에 사용할 {len(features)}개 특성 리스트 로드 완료.\")\n",
    "\n",
    "# ★★★ (핵심 수정 2) 하드코딩된 'features = [...]' 리스트 전체 삭제 ★★★\n",
    "# (이제 'features' 변수는 파일에서 자동으로 로드되므로 하드코딩된 리스트는 필요 없습니다)\n",
    "\n",
    "# --- 3. '공통 데이터'(test.csv) 로딩 및 전처리 ---\n",
    "print(f\"'{PROCESSED_TEST_FILE}' 파일을 불러옵니다...\")\n",
    "# '동적 마스크'에 헬퍼(Helper) 컬럼이 필요하므로 전체를 다 불러옵니다\n",
    "test_df_processed = pd.read_feather(PROCESSED_TEST_FILE)\n",
    "\n",
    "# ★★★ (핵심 수정 3) X_test는 파일에서 로드된 'features' 변수를 사용 ★★★\n",
    "X_test = test_df_processed[features]\n",
    "\n",
    "# (훈련(fit) 때와 동일하게 두 컬럼 모두 category로 변환)\n",
    "# (만약 'pv_id'나 'location_cluster'가 features 리스트에 없다면 이 부분은 자동으로 무시됩니다)\n",
    "if 'pv_id' in features:\n",
    "    X_test['pv_id'] = X_test['pv_id'].astype('category') \n",
    "if 'location_cluster' in features:\n",
    "    X_test['location_cluster'] = X_test['location_cluster'].astype('category') \n",
    "if 'climate_cluster' in features:\n",
    "    X_test['climate_cluster'] = X_test['climate_cluster'].astype('category')\n",
    "\n",
    "# --- 4. 최종 예측 수행 ---\n",
    "print(\"최종 예측을 수행합니다...\")\n",
    "predictions = final_model.predict(X_test)\n",
    "\n",
    "# --- 5. (필수) 후처리 2단계 적용 ---\n",
    "# 5A. (기본) 0보다 작은 값은 0으로 보정\n",
    "predictions[predictions < 0] = 0 \n",
    "\n",
    "# 5B. (핵심) '동적 0 처리' (안전망)\n",
    "print(\"개인화된 '일출/일몰 맵'을 기반으로 밤 시간대 예측을 0으로 강제 조정합니다...\")\n",
    "# 'processed_test.feather'에서 헬퍼(Helper) 컬럼을 가져옴\n",
    "decimal_hours = test_df_processed['decimal_hour'].to_numpy()\n",
    "sunrise_times = test_df_processed['sunrise_decimal'].to_numpy()\n",
    "sunset_times = test_df_processed['sunset_decimal'].to_numpy()\n",
    "\n",
    "# (훈련 코드와 동일한 로직) 혹시모를 안전장치인거지\n",
    "default_sunrise = 4.5\n",
    "default_sunset = 20.5\n",
    "sunrise_times[sunrise_times < -900] = default_sunrise \n",
    "sunset_times[sunset_times < -900] = default_sunset\n",
    "sunrise_times[sunrise_times == 0] = default_sunrise\n",
    "sunset_times[sunset_times == 0] = default_sunset\n",
    "\n",
    "pred_morning_mask = (decimal_hours <= sunrise_times)\n",
    "pred_evening_mask = (decimal_hours >= sunset_times)\n",
    "pred_night_mask = pred_morning_mask | pred_evening_mask\n",
    "\n",
    "# 예측(predictions) 배열에 0 처리 적용\n",
    "predictions[pred_night_mask] = 0\n",
    "print(\"동적 마스크 적용 완료.\")\n",
    "\n",
    "\n",
    "# --- 6. 제출 파일(submission.csv) 생성 ---\n",
    "print(\"제출 파일을 생성합니다...\")\n",
    "submission_df = pd.read_csv(SUBMISSION_TEMPLATE) \n",
    "submission_df['nins'] = predictions\n",
    "submission_df.to_csv(SUBMISSION_FILE, index=False)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"최종 제출 파일 '{SUBMISSION_FILE}' 생성이 완료되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
